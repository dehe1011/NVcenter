{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL agent for unitary gate building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to try and implement a RL agent (following various algorithms) that learn how to build a target unitary gate in a spin chain with control on the single qubits. The agent builds a parametrized circuit of N layers (with N initially fixed) and with constant structure (but variable parameters)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See also: https://github.com/ManuelGuatto/RL_4_Robust_QC/blob/main/PPO_Nominal_Damp.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.10.11 (v3.10.11:7d4cc5aa85, Apr  4 2023, 19:05:19) [Clang 13.0.0 (clang-1300.0.29.30)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(\"Python version:\", sys.version)\n",
    "import torch\n",
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import pennylane as qml\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import timeit\n",
    "import gymnasium as gym\n",
    "import stable_baselines3\n",
    "from gymnasium import spaces\n",
    "import qutip as q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Superoperator Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import starmap, product\n",
    "import numpy as np\n",
    "import functools as ft\n",
    "import scipy as sp\n",
    "# Initialization\n",
    "Id = np.eye(2)\n",
    "X = np.matrix([[0,1],[1,0]])\n",
    "Y = np.matrix([[0,-1j],[1j,0]])\n",
    "Z = np.matrix([[1,0],[0,-1]])\n",
    "\n",
    "\n",
    "Sigmam = (X+1j*Y)/2\n",
    "Sigmap = (X-1j*Y)/2\n",
    "# Multi-kroneker and matrix product functions\n",
    "# Multi-kronecker (tensor) product. Takes a list as argument and perform the tensor product of the elements\n",
    "def multikron(list):\n",
    "    a = ft.reduce(np.kron, list)\n",
    "    return a\n",
    "\n",
    "\n",
    "# Multi-matrix product. Takes a list and perform the dot product\n",
    "def multidot(list):\n",
    "    a = ft.reduce(np.dot, list)\n",
    "    return a\n",
    "#Define Puali basis for a generic number of qubits\n",
    "# Multi-qubit Pauli operator basis\n",
    "def pauli_basis_mine(nq):\n",
    "    sol = []\n",
    "    lst = list(product((Id,X,Y,Z),repeat=nq))\n",
    "    for i in range(len(lst)):\n",
    "        op = multikron(lst[i])\n",
    "        sol.append(op)\n",
    "    return sol\n",
    "# Transfer matrix, Puali transfer matrix, Choi matrix, and chi matrix superoperator representations for channles given their Krauss operators \n",
    "# Pauli transfer matrix for unitary operators: implement eq.75 in https://arxiv.org/abs/2408.12064 \n",
    "# \"Use PTMprocess with a list with one element to get the same result\"\n",
    "def PTM(unitary):\n",
    "    unitary = np.matrix(unitary)\n",
    "    nq = np.log2(len(unitary)).astype(np.int64)\n",
    "    B = np.zeros((4 ** nq, 4 ** nq), dtype=complex)\n",
    "\n",
    "    P = pauli_basis_mine(nq)\n",
    "    for i, opi in enumerate(P):\n",
    "        for j, opj in enumerate(P):\n",
    "            B[i][j] = np.trace(multidot([opi,unitary,opj,unitary.H]))/(2 ** nq)\n",
    "    return B\n",
    "\n",
    "\n",
    "# Compute the PTM for a process given in terms of Krauss operators, eq. 75 in https://arxiv.org/abs/2408.12064\n",
    "# \"Takes as input a list of Krauss operators\"\n",
    "def PTMprocess(krauss):\n",
    "    for k in range(len(krauss)):\n",
    "        krauss[k] = np.matrix(krauss[k])\n",
    "    nq = np.log2(len(krauss[0])).astype(np.int64)\n",
    "    B = np.zeros((4 ** nq, 4 ** nq), dtype=complex)\n",
    "\n",
    "    P = pauli_basis_mine(nq)\n",
    "    for i, opi in enumerate(P):\n",
    "        for j, opj in enumerate(P):\n",
    "            for k in range(len(krauss)):\n",
    "                B[i][j] += np.trace(multidot([opi,krauss[k],opj,krauss[k].H]))/(2 ** nq)\n",
    "    return B\n",
    "\n",
    "# Compute the state in the Pauli representation, eq.76 in https://arxiv.org/abs/2408.12064\n",
    "def PTMstate(rho):\n",
    "    rho = np.matrix(rho)\n",
    "    nq = np.log2(len(rho)).astype(np.int64)\n",
    "    B = np.zeros((4 ** nq, 1), dtype=complex)\n",
    "\n",
    "    P = pauli_basis_mine(nq)\n",
    "    for i, opi in enumerate(P):\n",
    "        B[i]= np.trace(multidot([opi,rho]))/(2 ** nq)\n",
    "    return B\n",
    "\n",
    "# Convert the state from the Pauli represenation back to the density matrix\n",
    "def PTMstate_to_rho(state):\n",
    "    nq = (0.5*np.log2(len(state))).astype(np.int64)\n",
    "    B = np.zeros((2 ** nq, 2 ** nq), dtype=complex)\n",
    "\n",
    "    P = pauli_basis_mine(nq)\n",
    "    for i, opi in enumerate(P):\n",
    "        B += state[i][0]*opi\n",
    "    return B\n",
    "\n",
    "# Compute the transition matrix in the unit basis, eq.73 in https://arxiv.org/abs/2408.12064\n",
    "def TMprocess(krauss):\n",
    "    for k in range(len(krauss)):\n",
    "        krauss[k] = np.matrix(krauss[k])\n",
    "    nq = np.log2(len(krauss[0])).astype(np.int64)\n",
    "    B = np.zeros((4 ** nq, 4 ** nq), dtype=complex)\n",
    "\n",
    "    \n",
    "    for k in range(len(krauss)):\n",
    "        B += np.kron(krauss[k].conjugate(),krauss[k])\n",
    "    return B\n",
    "\n",
    "# Perfrom the vectorization of a density matrix\n",
    "def TMstate(rho):\n",
    "    nq = np.log2(len(rho)).astype(np.int64)\n",
    "    B = np.reshape(rho,(4**nq,))\n",
    "    return B\n",
    "\n",
    "# Compute the process matrix of a quantum channel. Eq.88 in https://arxiv.org/abs/2408.12064\n",
    "def Chiprocess(krauss):\n",
    "    for k in range(len(krauss)):\n",
    "        krauss[k] = np.matrix(krauss[k])\n",
    "    nq = np.log2(len(krauss[0])).astype(np.int64)\n",
    "    B = np.zeros((4 ** nq, 4 ** nq), dtype=complex)\n",
    "\n",
    "    P = pauli_basis_mine(nq)\n",
    "    for i, opi in enumerate(P):\n",
    "        for j, opj in enumerate(P):\n",
    "            for m, opm in enumerate(P):\n",
    "                for k in range(len(krauss)):\n",
    "                    B[i][j] += np.trace(multidot([opm,opi,krauss[k],opm,krauss[k].H,opj]))/(8 ** nq)\n",
    "    return B\n",
    "\n",
    "# Vectorization of an operator\n",
    "def vec(rho):\n",
    "    nq = np.log2(len(rho)).astype(np.int64)\n",
    "    B = np.reshape(rho,(4**nq,))\n",
    "    return B\n",
    "\n",
    "# Compute the Choi matrix of a quantum channel. Eq.112 in https://arxiv.org/abs/2408.12064\n",
    "def Choi(krauss):\n",
    "    for k in range(len(krauss)):\n",
    "        krauss[k] = np.matrix(krauss[k])\n",
    "    nq = np.log2(len(krauss[0])).astype(np.int64)\n",
    "    \n",
    "    B = np.zeros((4 ** nq, 4 ** nq), dtype=complex)\n",
    "\n",
    "    for k in range(len(krauss)):\n",
    "        B += np.outer(vec(krauss[k]),vec(krauss[k]).conjugate())\n",
    "    return B\n",
    "# Compute the process fidelity between two unitaries, eq.237 in https://arxiv.org/abs/2408.12064\n",
    "def process_fid(un1,un2):\n",
    "    nq = np.log2(len(un1)).astype(np.int64)\n",
    "    fid = np.trace(np.dot(PTM(un1),np.linalg.inv(PTM(un2))))/(4**nq)\n",
    "    return fid\n",
    "\n",
    "# Generate an amplitude-phase damping superoperator channel applied to the qubit in position [qubit] in a system with num_qubit qubits\n",
    "# Ref. 10.1109/ACCESS.2020.3025619 \"Approximating Decoherence Processes for the Design and Simulation of Quantum Error\n",
    "#Correction Codes on Classical Computers\" https://ieeexplore.ieee.org/document/9201447\n",
    "\n",
    "def TM_APD(t,T1,T2, qubit, num_qubit):\n",
    "    #Karuss operators\n",
    "    gamma = 1-np.exp(-t/T1)\n",
    "    lam = 1-np.exp(t/T1-2*t/T2) \n",
    "\n",
    "    E0 = ((1+np.sqrt(1-gamma-(1-gamma)*lam))/2)*Id+((1-np.sqrt(1-gamma-(1-gamma)*lam))/2)*Z\n",
    "    E1 = (np.sqrt(gamma)/2)*X+ 1j*(np.sqrt(gamma)/2)*Y\n",
    "    E2 = (np.sqrt((1-gamma)*lam)/2)*Id-(np.sqrt((1-gamma)*lam)/2)*Z\n",
    "\n",
    "    list = [Id]*num_qubit\n",
    "\n",
    "    list[qubit] = E0\n",
    "    K0 = multikron(list)\n",
    "    list[qubit] = E1\n",
    "    K1 = multikron(list)\n",
    "    list[qubit] = E2\n",
    "    K2 = multikron(list)\n",
    "    \n",
    "\n",
    "    Eapd = [K0,K1,K2]\n",
    "    sol = TMprocess(Eapd)\n",
    "    return sol\n",
    "# Linblad superoperator from jump operators in the master equation:\n",
    "#See  https://journals.aps.org/prresearch/pdf/10.1103/PhysRevResearch.4.023216 (eq.3) and also     \n",
    "#https://doi.org/10.1063/1.1518555 \n",
    "# Generate the Linblad superoperator for a system of num_qubit qubits, with Hmiltonian h and list of jum operators L\n",
    "def Linbladian(h,L,num_qubit):\n",
    "    Ide = multikron([Id]*num_qubit)\n",
    "    miH = -1j*(multikron([Ide,h])-multikron([h.conjugate(),Ide]))\n",
    "    \n",
    "    mF = np.zeros((4 ** num_qubit, 4 ** num_qubit), dtype=complex)\n",
    "    for k in range(len(L)):\n",
    "        mF += multikron([L[k].conjugate(),L[k]])-0.5* multikron([Ide,multidot([L[k].conjugate().T,L[k]])])-0.5* multikron([multidot([L[k].T,L[k].conjugate()]),Ide])\n",
    "    return miH+mF\n",
    "\n",
    "# Exponentiate the Linblad superoperator to obatin the quantum channel generated by it\n",
    "def LinbladianExp(t,h,L,num_qubit):\n",
    "    sol = sp.linalg.expm(t*Linbladian(h,L,num_qubit))\n",
    "\n",
    "    return sol\n",
    "# Generate the amplitude-phase damping jump operators for the qubit in position [qubit] in a system with num_qubit qubits\n",
    "def Linb_APD(T1,T2, qubit, num_qubit):\n",
    "    #Karuss operators\n",
    "    gammaR = 1/T1\n",
    "    gammaD = (-1/T1+2/T2)/2 \n",
    "\n",
    "    \n",
    "    L1 = np.sqrt(gammaD/2)*Z\n",
    "    L2 = np.sqrt(gammaR)*Sigmam\n",
    "    \n",
    "\n",
    "    list = [Id]*num_qubit\n",
    "\n",
    "    list[qubit] = L1\n",
    "    L1m = multikron(list)\n",
    "    list[qubit] = L2\n",
    "    L2m = multikron(list)\n",
    "    \n",
    "    Eapd = [L1m,L2m]\n",
    "\n",
    "    return Eapd\n",
    "\n",
    "# Visualization aid for matrices\n",
    "#from qiskit.visualization import array_to_latex\n",
    "\n",
    "\n",
    "# Generate the Linblad superoperator for a system of num_qubit qubits, with Hmiltonian h and Linblad generator super operator L\n",
    "def LinbladianV2(h,L,num_qubit):\n",
    "    Ide = multikron([Id]*num_qubit)\n",
    "    miH = -1j*(multikron([Ide,h])-multikron([h.conjugate(),Ide]))\n",
    "    \n",
    "    mF = L\n",
    "    return miH+mF\n",
    "\n",
    "# Exponentiate the Linblad superoperator to obatin the quantum channel generated by it\n",
    "def LinbladianExpV2(t,h,L,num_qubit):\n",
    "    sol = sp.linalg.expm(t*LinbladianV2(h,L,num_qubit))\n",
    "\n",
    "    return sol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.49637164+0.j,  0.        +0.j,  0.        +0.j,\n",
       "         0.        +0.j],\n",
       "       [ 0.        +0.j,  0.49637164+0.j,  0.        +0.j,\n",
       "         0.        +0.j],\n",
       "       [ 0.        +0.j,  0.        +0.j, -0.01884956+0.j,\n",
       "        -0.34557519+0.j],\n",
       "       [ 0.        +0.j,  0.        +0.j, -0.34557519+0.j,\n",
       "         0.01884956+0.j]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_wires = 2\n",
    "\n",
    "Xp, Yp, Zp = qml.PauliX, qml.PauliY, qml.PauliZ\n",
    "\n",
    "proj_0 = 1/2*(Zp(wires= [0]) + qml.Identity(wires=[0]))\n",
    "proj_1 = 1/2*(-Zp(wires= [0]) + qml.Identity(wires=[0]))\n",
    "\n",
    "\n",
    "Idp = qml.Identity(wires=[0,1]).matrix()\n",
    "#param0 = -0.158\n",
    "#param1 = 0.110\n",
    "#param2 = -(0.158+0.152)\n",
    "\n",
    "ops =  [proj_0 @ (Zp(1)/2),proj_1 @ (Zp(1)/2),proj_1 @ (Xp(1)/2)]\n",
    "\n",
    "couplings = [-2*np.pi*0.158,-2*np.pi*(0.158-0.152),-2*np.pi*0.110]\n",
    "\n",
    "Ham = qml.dot(couplings,ops).matrix()\n",
    "\n",
    "Ham2 = qml.dot(couplings,ops)\n",
    "\n",
    "\n",
    "\n",
    "def profit(op_mat,target_mat):\n",
    "    \"\"\"Compute the fidelity function for given parameters.\"\"\"\n",
    "    #op_mat = param_circ(Nfree,params)\n",
    "    # Compute the fidelity between the target and the gate\n",
    "    return np.abs(np.trace(target_mat.conj().T @ ( op_mat))) / 2**num_wires\n",
    "#np.abs(np.trace(target_mat.conj().T @ op_mat)) / np.abs(np.trace(target_mat.conj().T @ target_mat)) #\n",
    "\n",
    "# Version without Super - Operators for observation space\n",
    "def Ufree(T):\n",
    "    evo = qml.exp(Ham2, -1j * T)\n",
    "    return evo.matrix()\n",
    "\n",
    "def UpulseV2(params):\n",
    "    # parameters are: [time interval duration, phase]\n",
    "    \n",
    "    rabi_f = 0.5*2*np.pi\n",
    "    coeff=[rabi_f*np.cos(params[1]),rabi_f*np.sin(params[1])]\n",
    "\n",
    "    op = [Xp(0) @ qml.Identity(1),Yp(0) @ qml.Identity(1)]\n",
    "    Hmw = qml.dot(coeff,op)\n",
    "    \n",
    "    Htot = Ham2+Hmw\n",
    "    \n",
    "    deltaT = np.abs(params[0])\n",
    "\n",
    "    evo = qml.exp(Htot, -1j * deltaT)\n",
    "    \n",
    "    return evo.matrix()\n",
    "\n",
    "\n",
    "# We want to add some amplitude-phase damping on the electron qubit with characteristic times T1 and T2 to both the free evolution and the evolution with pulses\n",
    "def UfreeSuper(T):\n",
    "    h = Ham\n",
    "    T1 = T1env # 6e3 microseconds\n",
    "    T2 = T2env # Pure dephasing time in microseconds as in Suter Grover algorithm implementation\n",
    "    num_qubit = num_wires\n",
    "    L = Linb_APD(T1,T2, 0, num_qubit)\n",
    "    evo = LinbladianExp(T,h,L,num_qubit)\n",
    "    #evo = qml.exp(H, -1j * T)\n",
    "    return evo\n",
    "\n",
    "def UpulseSuper(params):\n",
    "    # parameters are: [time interval duration, phase]\n",
    "    \n",
    "    rabi_f = 0.5*2*np.pi\n",
    "    coeff=[rabi_f*np.cos(params[1])/2,rabi_f*np.sin(params[1])/2]\n",
    "\n",
    "    op = [Xp(0) @ qml.Identity(1),Yp(0) @ qml.Identity(1)]\n",
    "    Hmw = qml.dot(coeff,op).matrix()\n",
    "    \n",
    "    Htot = Ham+Hmw\n",
    "\n",
    "    T1 = T1env # 6e3 microseconds\n",
    "    T2 = T2env # Pure dephasing time in microseconds as in Suter Grover algorithm implementation\n",
    "    num_qubit = num_wires\n",
    "    L = Linb_APD(T1,T2, 0, num_qubit)\n",
    "    \n",
    "    deltaT = params[0]\n",
    "\n",
    "    evo = LinbladianExp(deltaT,Htot,L,num_qubit)\n",
    "\n",
    "    #evo = qml.exp(Htot, -1j * deltaT)\n",
    "    \n",
    "    return evo\n",
    "#16*16\n",
    "\n",
    "Ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_suter_CNOT = [3.78,1.88,0.0 , 2.11,3.96 ,np.pi/5 ,2.15,1.9 ,np.pi/2 ,0.63]\n",
    "target2 = (proj_0 @ qml.Identity(1)-1j*proj_1 @ Xp(1)).matrix()\n",
    "target3 = np.array([[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 0, -1j], [0, 0, -1j, 0]])\n",
    "\n",
    "#profit(suter_CNOT(params_suter_CNOT,3),target3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def suter_layer(params):\n",
    "    # Here we can define a layer as an evolution without pulse control follow by a pulse control with fixed Rabi frequency\n",
    "    U = UpulseV2([params[1],params[2]]) @ Ufree(params[0])\n",
    "    return U\n",
    "\n",
    "def suter_layer_Super(params):\n",
    "    # Here we can define a layer as an evolution without pulse control follow by a pulse control with fixed Rabi frequency\n",
    "    U = UpulseSuper([params[1],params[2]]) @ UfreeSuper(params[0])\n",
    "    return U\n",
    "\n",
    "def suter_hadamard_Super(params,N):\n",
    "    U = multikron([Id]*(2*num_wires))\n",
    "    for i in range(N):\n",
    "        para = params[0+3*i:3+3*i]\n",
    "        U = suter_layer_Super(para) @ U\n",
    "    U = UfreeSuper(params[-1]) @ U\n",
    "    return U\n",
    "\n",
    "def profitSuper(op_mat,target_mat):\n",
    "    \"\"\"Compute the fidelity function for given parameters.\"\"\"\n",
    "    #op_mat = param_circ(Nfree,params)\n",
    "    # Compute the fidelity between the target and the gate\n",
    "    return np.abs(np.trace(target_mat.conj().T @ ( op_mat))) / np.abs(np.trace(target_mat.conj().T @ target_mat))\n",
    "\n",
    "####\n",
    "\n",
    "# We want to add some amplitude-phase damping on the electron qubit with characteristic times T1 and T2 to both the free evolution and the evolution with pulses\n",
    "def UfreeSuper_e(T,T1,T2):\n",
    "    h = Ham\n",
    "    #T1 = 6000 # 6e3 microseconds\n",
    "    #T2 = 35 # Pure dephasing time in microseconds as in Suter Grover algorithm implementation\n",
    "    num_qubit = num_wires\n",
    "    L = Linb_APD(T1,T2, 0, num_qubit)\n",
    "    evo = LinbladianExp(T,h,L,num_qubit)\n",
    "    #evo = qml.exp(H, -1j * T)\n",
    "    return evo\n",
    "\n",
    "\n",
    "###to compare with suter parameters#### Definition with _e at the end!\n",
    "def UpulseSuper_e(params,T1,T2):\n",
    "    # parameters are: [time interval duration, phase]\n",
    "    \n",
    "    rabi_f = 0.5*2*np.pi\n",
    "    coeff=[rabi_f*np.cos(params[1])/2,rabi_f*np.sin(params[1])/2]\n",
    "\n",
    "    op = [Xp(0) @ qml.Identity(1),Yp(0) @ qml.Identity(1)]\n",
    "    Hmw = qml.dot(coeff,op).matrix()\n",
    "    \n",
    "    Htot = Ham+Hmw\n",
    "\n",
    "    #T1 = 6000 # 6e3 microseconds\n",
    "    #T2 = 35 # Pure dephasing time in microseconds as in Suter Grover algorithm implementation\n",
    "    num_qubit = num_wires\n",
    "    L = Linb_APD(T1,T2, 0, num_qubit)\n",
    "    \n",
    "    deltaT = params[0]\n",
    "\n",
    "    evo = LinbladianExp(deltaT,Htot,L,num_qubit)\n",
    "\n",
    "    #evo = qml.exp(Htot, -1j * deltaT)\n",
    "    \n",
    "    return evo\n",
    "\n",
    "def suter_layer_Super_e(params,T1,T2):\n",
    "    # Here we can define a layer as an evolution without pulse control follow by a pulse control with fixed Rabi frequency\n",
    "    U = UpulseSuper_e([params[1],params[2]],T1,T2) @ UfreeSuper_e(params[0],T1,T2)\n",
    "    return U\n",
    "    \n",
    "def suter_circ_Super(params,N,T1,T2):\n",
    "    U = multikron([Id]*(2*num_wires))\n",
    "    for i in range(N):\n",
    "        para = params[0+3*i:3+3*i]\n",
    "        U = suter_layer_Super_e(para,T1,T2) @ U\n",
    "    U = UfreeSuper_e(params[-1],T1,T2) @ U\n",
    "    return U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suter_CNOT_Super2(params,N):\n",
    "    U = UfreeSuper(0)\n",
    "    for i in range(N):\n",
    "        para = params[0+3*i:3+3*i]\n",
    "        U = suter_layer_Super(para) @ U\n",
    "    U = UfreeSuper(params[-1]) @ U\n",
    "    return U\n",
    "\n",
    "#suter_layer([1.1,1.1,np.pi])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8763900930360686"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T1env = 6000\n",
    "T2env = 35\n",
    "\n",
    "params_suter_CNOT = [3.78,1.88,0.0 , 2.11,3.96 ,np.pi/5 ,2.15,1.9 ,np.pi/2 ,0.63]\n",
    "params_suter_Hadamard = [0.74,0.23,3*np.pi/2 , 0.22,1.26 ,3*np.pi/2 ,0.43,1.5 ,np.pi/2 ,0.89]\n",
    "\n",
    "target3  = (qml.Identity(0) @ qml.Hadamard(wires=[1])).matrix()\n",
    "\n",
    "target2SuperCNOT = TMprocess([target2])\n",
    "target3SuperHad = TMprocess([target3])\n",
    "profitSuper(suter_circ_Super(params_suter_Hadamard,3,T1env,T2env),target3SuperHad)\n",
    "#profitSuper(suter_layer_Super([1.1,1.1,np.pi]),TMprocess([Idp]))\n",
    "#target2Super"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(PTM([Idp]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parametric_envL(gym.Env):\n",
    "    \"\"\"\n",
    "    Custom Environment that follows gym interface.\n",
    "    \"\"\"\n",
    "\n",
    "    #metadata = {\"render_modes\": [\"console\"]}\n",
    "\n",
    "    MAX_STEPS = 2\n",
    "    INFIDELITY_THRESHOLD = 0.8\n",
    "\n",
    "\n",
    "    def __init__(self, env_conf):\n",
    "        #super(Parametric_env, self).__init__()\n",
    "        #self.render_mode = render_mode\n",
    "        #target2Super = TMprocess([target2])\n",
    "\n",
    "        self.target = env_conf[\"Target\"] #qml.Toffoli([0, 1, 2]).matrix()\n",
    "        self.alpha = env_conf[\"Lagrange_time\"]\n",
    "\n",
    "        self.U = UfreeSuper(0)\n",
    "        self.Utest = Ufree(0)\n",
    "\n",
    "        #Define the action and observation spaces\n",
    "        #self.action_space = spaces.Box(low=0, high=np.array([0.02, \n",
    "        #                                                     2*np.pi, 2*np.pi, 2*np.pi, 2*np.pi, \n",
    "        #                                                     2*np.pi, 2*np.pi, 2*np.pi, 2*np.pi, \n",
    "        #                                                     2*np.pi]), dtype=np.float32)\n",
    "        #Here we normalize the action space\n",
    "        self.action_space = spaces.Box(low=-1, high= 1, shape=(3,), dtype=np.float32)\n",
    "        \n",
    "        #self.action_space = spaces.Dict(\n",
    "        #    {\n",
    "        #        \"angle_Z\": spaces.Discrete(5, start=-2, seed=42),\n",
    "        #        \"angle_X\": spaces.Discrete(5, start=-2, seed=42),\n",
    "        #    }\n",
    "        #)\n",
    "        #self.action_space = spaces.MultiDiscrete([5,5],start=[-2,-2])\n",
    "        #self.action_space = spaces.MultiDiscrete([10,9,9,9])\n",
    "      #  self.observation_space = spaces.Box(low=-1, high=1, shape=(512,), dtype=np.float64)\n",
    "        self.observation_space = spaces.Box(low=-1, high=1, shape=(32,), dtype=np.float64)\n",
    "\n",
    "        #self.observation_space = spaces.Dict(\n",
    "        #    {\n",
    "        #        \"real_part\": spaces.Box(low=-1, high=1, shape=(4,), dtype=np.float64),\n",
    "        #        \"imaginary_part\": spaces.Box(low=-1, high=1, shape=(4,), dtype=np.float64),\n",
    "        #    }\n",
    "        #)\n",
    "\n",
    "    # Function that gets the state and gives the observation in the correct format\n",
    "    def _get_obs(self):\n",
    "        #return {\"real_part\": np.real(self.U), \"imaginary_part\": np.imag(self.U)}\n",
    "       # obs = np.concatenate([np.real(self.U.reshape(256,)),np.imag(self.U.reshape(256,))])\n",
    "        obs = np.concatenate([np.real(self.Utest.reshape(16,)),np.imag(self.Utest.reshape(16,))])\n",
    "        return obs\n",
    "    #def _get_obs(self):\n",
    "    #    #return {\"real_part\": np.real(self.U), \"imaginary_part\": np.imag(self.U)}\n",
    "    #    return {\"real_part\": np.real(self.U.reshape(4,)), \"imaginary_part\": np.imag(self.U.reshape(4,))}\n",
    "    \n",
    "    def reset(self,seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.U = UfreeSuper(0)\n",
    "        self.Utest = Ufree(0)\n",
    "       # self.Uobs = \n",
    "        #self.rho_hat = self.rho\n",
    "        #self.true_fidelity = 0\n",
    "        self.fidelity = profitSuper(self.U,self.target)#0\n",
    "        self.count = 0\n",
    "        self.reward = 0\n",
    "        self.done = False\n",
    "        self.duration = 0\n",
    "        observation = self._get_obs()\n",
    "        self.info = {}\n",
    "\n",
    "        return observation,{}\n",
    "    \n",
    "    # Function to convert the action sampled from action_space into angles for the rotations in the layer\n",
    "    def _get_angle(self,para):\n",
    "        az = 2*np.pi*para      \n",
    "        return az\n",
    "    \n",
    "    # Function to renormlize the times in between [0,10]\n",
    "    def _get_times(self,para):\n",
    "        az = 2.0*np.abs(para) + np.abs(0.1)  #TIMES##########################################\n",
    "        \n",
    "        return az\n",
    "    # Function to renormlize the times in between [0,10]\n",
    "    def _get_times2(self,para):\n",
    "        az = 2.2*np.abs(para) + np.abs(0.3)  #TIMES##########################################\n",
    "        \n",
    "        return az\n",
    "\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        #Check the state of the episode\n",
    "        if self.done:\n",
    "            print(\"EPISODE DONE!!!\")\n",
    "        elif (self.count == self.MAX_STEPS):\n",
    "            self.done = True;\n",
    "            #truncated = self.done; \n",
    "        else:\n",
    "            assert self.action_space.contains(action)\n",
    "            self.count += 1\n",
    "            #self.duration += 4*np.pi*np.abs(action[0])\n",
    "            \n",
    "            \n",
    "        #Define the effect of the unitary layer\n",
    "        self.params = [self._get_times(action[0]),self._get_times(action[1]),self._get_angle(action[2])] \n",
    "        #print(self.params)\n",
    "        op =  suter_layer_Super(self.params)  ###CHANGE: HERE\n",
    "        opobs = suter_layer(self.params)\n",
    "       # print(q.Qobj(op))\n",
    "       # Upre = self.U\n",
    "        Uobs = self.Utest\n",
    "\n",
    "        self.U = op @ self.U                  ##CHANGE: HERE\n",
    "        self.Utest = opobs @ self.Utest###!\n",
    "        #print(\"unitary=\", self.Utest)\n",
    "\n",
    "        #Compute the reward\n",
    "        #fid0 = self.fidelity\n",
    "        #fid = profit(self.U,self.target)\n",
    "        #fid = profit2(self.U,self.target,self.alpha,self.duration)\n",
    "        self.fidelity = profitSuper(self.U,self.target)  ###CHANGE: HERE\n",
    "        #self.fidelity = profit(self.Utest, target3)  ###CHANGE: HERE\n",
    "       # print(self.fidelity)\n",
    "        #if self.done:\n",
    "        #    self.reward = self.fidelity\n",
    "        #else:\n",
    "        #    self.reward = 0\n",
    "        #self.reward = fid#-fid0#self.fidelity\n",
    "        self.info = {\"Fidelity\": self.fidelity,\"Pre\":Uobs,\"Next\":self.Utest}\n",
    "\n",
    "        if 1-self.fidelity < self.INFIDELITY_THRESHOLD:\n",
    "            self.done = True\n",
    "\n",
    "        # We give reward only at the end of the episode\n",
    "        if self.done:\n",
    "            self.reward =   -np.log(1-self.fidelity) #-0.05*self.count\n",
    "        else:\n",
    "            self.reward = 0\n",
    "\n",
    "        #Rewavrds at each step\n",
    "        #self.reward = self.fidelity - fid0\n",
    "        observation = self._get_obs()\n",
    "\n",
    "        \n",
    "        #try:\n",
    "        #    assert self.observation_space.contains(self.rho)\n",
    "        #except AssertionError:\n",
    "        #    print(\"INVALID STATE\", self.rho)\n",
    "        #terminated = self.done\n",
    "\n",
    "       # print(profit2)\n",
    "        return (observation, self.reward, self.done,self.done, self.info)\n",
    "\n",
    "   \n",
    "\n",
    "        \n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "#test = q.rand_unitary(16, density=0.75, dims=None)\n",
    "#test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test = q.rand_unitary(16, density=0.75, dims=None)\n",
    "#profitSuper(test.full(),target2SuperCNOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://optuna.github.io/optuna-dashboard/\n",
    "#https://optuna-dashboard.readthedocs.io/en/stable/getting-started.html\n",
    "\n",
    "\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "rho_target = UfreeSuper(0)\n",
    "#target2Super = TMprocess([target2])\n",
    "alpha = 1\n",
    "\n",
    "env = Parametric_envL(env_conf={\"Target\":rho_target,\"Lagrange_time\":alpha})\n",
    "# If the environment don't follow the interface, an error will be thrown\n",
    "check_env(env, warn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO, A2C, DQN, TD3, DDPG\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "#target2Super = TMprocess([target3])\n",
    "# Instantiate the env\n",
    "#env_conf=dict(Target=Id,Lagrange_time=alpha)\n",
    "\n",
    "vec_env = make_vec_env(lambda:Parametric_envL(env_conf={\"Target\": target3SuperHad,\"Lagrange_time\":alpha})\n",
    ", n_envs=1)\n",
    "\n",
    "#vec_env = make_vec_env(Parametric_env, n_envs=1, env_kwargs=dict(Target=Id,Lagrange_time=alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1e+03 ns, sys: 1 µs, total: 2 µs\n",
      "Wall time: 2.62 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "numberofpulses = 0\n",
    "t1 = 2\n",
    "t2 = 16\n",
    "T1env = 6000\n",
    "T2env = 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to ./ppo_cartpole_tensorboard/diss3/PPO_57\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1.77     |\n",
      "|    ep_rew_mean     | 0.44     |\n",
      "| time/              |          |\n",
      "|    fps             | 121      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 8        |\n",
      "|    total_timesteps | 1024     |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1.68          |\n",
      "|    ep_rew_mean          | 0.489         |\n",
      "| time/                   |               |\n",
      "|    fps                  | 103           |\n",
      "|    iterations           | 2             |\n",
      "|    time_elapsed         | 19            |\n",
      "|    total_timesteps      | 2048          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00043403532 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.3           |\n",
      "|    entropy_loss         | -4.26         |\n",
      "|    explained_variance   | -1.88         |\n",
      "|    learning_rate        | 5e-05         |\n",
      "|    loss                 | 0.156         |\n",
      "|    n_updates            | 10            |\n",
      "|    policy_gradient_loss | -0.0018       |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 0.481         |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1.81          |\n",
      "|    ep_rew_mean          | 0.437         |\n",
      "| time/                   |               |\n",
      "|    fps                  | 107           |\n",
      "|    iterations           | 3             |\n",
      "|    time_elapsed         | 28            |\n",
      "|    total_timesteps      | 3072          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00018766505 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.3           |\n",
      "|    entropy_loss         | -4.26         |\n",
      "|    explained_variance   | -0.673        |\n",
      "|    learning_rate        | 5e-05         |\n",
      "|    loss                 | 0.0815        |\n",
      "|    n_updates            | 20            |\n",
      "|    policy_gradient_loss | -0.00134      |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 0.245         |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.61         |\n",
      "|    ep_rew_mean          | 0.46         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 105          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 38           |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006445104 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.3          |\n",
      "|    entropy_loss         | -4.26        |\n",
      "|    explained_variance   | -0.361       |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 0.0393       |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00245     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 0.138        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.78         |\n",
      "|    ep_rew_mean          | 0.443        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 103          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 49           |\n",
      "|    total_timesteps      | 5120         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003417555 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.3          |\n",
      "|    entropy_loss         | -4.27        |\n",
      "|    explained_variance   | -0.0927      |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 0.0311       |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00185     |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 0.102        |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1.82          |\n",
      "|    ep_rew_mean          | 0.477         |\n",
      "| time/                   |               |\n",
      "|    fps                  | 105           |\n",
      "|    iterations           | 6             |\n",
      "|    time_elapsed         | 58            |\n",
      "|    total_timesteps      | 6144          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00016816065 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.3           |\n",
      "|    entropy_loss         | -4.28         |\n",
      "|    explained_variance   | -0.0327       |\n",
      "|    learning_rate        | 5e-05         |\n",
      "|    loss                 | 0.0273        |\n",
      "|    n_updates            | 50            |\n",
      "|    policy_gradient_loss | -0.00132      |\n",
      "|    std                  | 1.01          |\n",
      "|    value_loss           | 0.0954        |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.68        |\n",
      "|    ep_rew_mean          | 0.462       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 106         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 67          |\n",
      "|    total_timesteps      | 7168        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.000841745 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -4.28       |\n",
      "|    explained_variance   | 0.00357     |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 0.0256      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.00225    |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 0.0989      |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 1.63          |\n",
      "|    ep_rew_mean          | 0.464         |\n",
      "| time/                   |               |\n",
      "|    fps                  | 105           |\n",
      "|    iterations           | 8             |\n",
      "|    time_elapsed         | 77            |\n",
      "|    total_timesteps      | 8192          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00052673585 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.3           |\n",
      "|    entropy_loss         | -4.28         |\n",
      "|    explained_variance   | 0.0256        |\n",
      "|    learning_rate        | 5e-05         |\n",
      "|    loss                 | 0.0279        |\n",
      "|    n_updates            | 70            |\n",
      "|    policy_gradient_loss | -0.0014       |\n",
      "|    std                  | 1.01          |\n",
      "|    value_loss           | 0.0921        |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.61         |\n",
      "|    ep_rew_mean          | 0.497        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 101          |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 90           |\n",
      "|    total_timesteps      | 9216         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017623298 |\n",
      "|    clip_fraction        | 0.000195     |\n",
      "|    clip_range           | 0.3          |\n",
      "|    entropy_loss         | -4.29        |\n",
      "|    explained_variance   | 0.11         |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 0.0124       |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.00448     |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 0.0817       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.6          |\n",
      "|    ep_rew_mean          | 0.495        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 100          |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 101          |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033173144 |\n",
      "|    clip_fraction        | 0.000195     |\n",
      "|    clip_range           | 0.3          |\n",
      "|    entropy_loss         | -4.29        |\n",
      "|    explained_variance   | 0.123        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | 0.00937      |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.00624     |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 0.0866       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.85        |\n",
      "|    ep_rew_mean          | 0.43        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 101         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 110         |\n",
      "|    total_timesteps      | 11264       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006937485 |\n",
      "|    clip_fraction        | 0.00508     |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -4.29       |\n",
      "|    explained_variance   | 0.173       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 0.0101      |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.011      |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 0.0821      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.66        |\n",
      "|    ep_rew_mean          | 0.51        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 102         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 119         |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010564453 |\n",
      "|    clip_fraction        | 0.0129      |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -4.3        |\n",
      "|    explained_variance   | 0.185       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.000443   |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.021      |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 0.0852      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.49        |\n",
      "|    ep_rew_mean          | 0.542       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 103         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 128         |\n",
      "|    total_timesteps      | 13312       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012206217 |\n",
      "|    clip_fraction        | 0.0146      |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -4.3        |\n",
      "|    explained_variance   | 0.225       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0293     |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0313     |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 0.0825      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.38        |\n",
      "|    ep_rew_mean          | 0.627       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 104         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 137         |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012207342 |\n",
      "|    clip_fraction        | 0.0147      |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -4.3        |\n",
      "|    explained_variance   | 0.233       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0453     |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0356     |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 0.083       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.44        |\n",
      "|    ep_rew_mean          | 0.568       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 104         |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 147         |\n",
      "|    total_timesteps      | 15360       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014611279 |\n",
      "|    clip_fraction        | 0.0203      |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -4.3        |\n",
      "|    explained_variance   | 0.248       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.047      |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0381     |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 0.0843      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.22        |\n",
      "|    ep_rew_mean          | 0.657       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 104         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 156         |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014285935 |\n",
      "|    clip_fraction        | 0.0182      |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -4.3        |\n",
      "|    explained_variance   | 0.287       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0504     |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0416     |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 0.0813      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.27        |\n",
      "|    ep_rew_mean          | 0.636       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 104         |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 166         |\n",
      "|    total_timesteps      | 17408       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015471226 |\n",
      "|    clip_fraction        | 0.0189      |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -4.3        |\n",
      "|    explained_variance   | 0.332       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.051      |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0413     |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 0.0741      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.17        |\n",
      "|    ep_rew_mean          | 0.706       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 105         |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 175         |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014303446 |\n",
      "|    clip_fraction        | 0.0185      |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -4.3        |\n",
      "|    explained_variance   | 0.373       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0401     |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.038      |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 0.0685      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1.12       |\n",
      "|    ep_rew_mean          | 0.715      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 105        |\n",
      "|    iterations           | 19         |\n",
      "|    time_elapsed         | 184        |\n",
      "|    total_timesteps      | 19456      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01873061 |\n",
      "|    clip_fraction        | 0.0288     |\n",
      "|    clip_range           | 0.3        |\n",
      "|    entropy_loss         | -4.31      |\n",
      "|    explained_variance   | 0.355      |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | -0.046     |\n",
      "|    n_updates            | 180        |\n",
      "|    policy_gradient_loss | -0.0399    |\n",
      "|    std                  | 1.02       |\n",
      "|    value_loss           | 0.059      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.02        |\n",
      "|    ep_rew_mean          | 0.752       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 105         |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 194         |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016668724 |\n",
      "|    clip_fraction        | 0.0254      |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -4.31       |\n",
      "|    explained_variance   | 0.349       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0305     |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0312     |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 0.0527      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.07        |\n",
      "|    ep_rew_mean          | 0.759       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 105         |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 204         |\n",
      "|    total_timesteps      | 21504       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017802725 |\n",
      "|    clip_fraction        | 0.0256      |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -4.31       |\n",
      "|    explained_variance   | 0.337       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0374     |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0304     |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 0.0428      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.08        |\n",
      "|    ep_rew_mean          | 0.74        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 105         |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 214         |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021341508 |\n",
      "|    clip_fraction        | 0.037       |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -4.31       |\n",
      "|    explained_variance   | 0.3         |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0375     |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.0256     |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 0.0371      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.04        |\n",
      "|    ep_rew_mean          | 0.752       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 104         |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 225         |\n",
      "|    total_timesteps      | 23552       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034442432 |\n",
      "|    clip_fraction        | 0.0694      |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -4.31       |\n",
      "|    explained_variance   | 0.272       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.066      |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0404     |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 0.0347      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1.04       |\n",
      "|    ep_rew_mean          | 0.803      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 104        |\n",
      "|    iterations           | 24         |\n",
      "|    time_elapsed         | 234        |\n",
      "|    total_timesteps      | 24576      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03733066 |\n",
      "|    clip_fraction        | 0.0789     |\n",
      "|    clip_range           | 0.3        |\n",
      "|    entropy_loss         | -4.31      |\n",
      "|    explained_variance   | 0.255      |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | -0.0919    |\n",
      "|    n_updates            | 230        |\n",
      "|    policy_gradient_loss | -0.0535    |\n",
      "|    std                  | 1.02       |\n",
      "|    value_loss           | 0.0277     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1          |\n",
      "|    ep_rew_mean          | 0.827      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 105        |\n",
      "|    iterations           | 25         |\n",
      "|    time_elapsed         | 243        |\n",
      "|    total_timesteps      | 25600      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03453047 |\n",
      "|    clip_fraction        | 0.0737     |\n",
      "|    clip_range           | 0.3        |\n",
      "|    entropy_loss         | -4.31      |\n",
      "|    explained_variance   | 0.389      |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | -0.0896    |\n",
      "|    n_updates            | 240        |\n",
      "|    policy_gradient_loss | -0.0497    |\n",
      "|    std                  | 1.02       |\n",
      "|    value_loss           | 0.0246     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1.04       |\n",
      "|    ep_rew_mean          | 0.822      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 104        |\n",
      "|    iterations           | 26         |\n",
      "|    time_elapsed         | 254        |\n",
      "|    total_timesteps      | 26624      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02675255 |\n",
      "|    clip_fraction        | 0.0506     |\n",
      "|    clip_range           | 0.3        |\n",
      "|    entropy_loss         | -4.31      |\n",
      "|    explained_variance   | 0.388      |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | -0.0922    |\n",
      "|    n_updates            | 250        |\n",
      "|    policy_gradient_loss | -0.0471    |\n",
      "|    std                  | 1.02       |\n",
      "|    value_loss           | 0.0238     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 0.832       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 105         |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 262         |\n",
      "|    total_timesteps      | 27648       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025142029 |\n",
      "|    clip_fraction        | 0.0424      |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -4.31       |\n",
      "|    explained_variance   | 0.37        |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0835     |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0448     |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 0.017       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 0.851       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 107         |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 267         |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025641408 |\n",
      "|    clip_fraction        | 0.0475      |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -4.3        |\n",
      "|    explained_variance   | 0.255       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0836     |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.0453     |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 0.0123      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 0.86        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 109         |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 272         |\n",
      "|    total_timesteps      | 29696       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021845596 |\n",
      "|    clip_fraction        | 0.0333      |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -4.3        |\n",
      "|    explained_variance   | 0.269       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0842     |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0381     |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 0.00727     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 0.862       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 111         |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 276         |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016607862 |\n",
      "|    clip_fraction        | 0.0214      |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -4.3        |\n",
      "|    explained_variance   | 0.431       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0505     |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.0239     |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 0.007       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 0.863       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 112         |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 282         |\n",
      "|    total_timesteps      | 31744       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018282928 |\n",
      "|    clip_fraction        | 0.0272      |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -4.29       |\n",
      "|    explained_variance   | 0.299       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0503     |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0235     |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 0.00342     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 0.853       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 114         |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 286         |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013452397 |\n",
      "|    clip_fraction        | 0.0136      |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -4.29       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0205     |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.0183     |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 0.00123     |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1            |\n",
      "|    ep_rew_mean          | 0.865        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 114          |\n",
      "|    iterations           | 33           |\n",
      "|    time_elapsed         | 294          |\n",
      "|    total_timesteps      | 33792        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0128949545 |\n",
      "|    clip_fraction        | 0.0126       |\n",
      "|    clip_range           | 0.3          |\n",
      "|    entropy_loss         | -4.29        |\n",
      "|    explained_variance   | 0.124        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | -0.0516      |\n",
      "|    n_updates            | 320          |\n",
      "|    policy_gradient_loss | -0.0181      |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 0.00105      |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.02        |\n",
      "|    ep_rew_mean          | 0.855       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 116         |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 298         |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011709819 |\n",
      "|    clip_fraction        | 0.00752     |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -4.29       |\n",
      "|    explained_variance   | 0.36        |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0378     |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.0108     |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 0.00153     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 0.864       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 118         |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 302         |\n",
      "|    total_timesteps      | 35840       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007339331 |\n",
      "|    clip_fraction        | 0.00215     |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -4.28       |\n",
      "|    explained_variance   | 0.505       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0248     |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.00746    |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 0.00134     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.02        |\n",
      "|    ep_rew_mean          | 0.857       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 119         |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 309         |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007109132 |\n",
      "|    clip_fraction        | 0.00215     |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -4.28       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | 0.0119      |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.0068     |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 0.000564    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.02         |\n",
      "|    ep_rew_mean          | 0.853        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 120          |\n",
      "|    iterations           | 37           |\n",
      "|    time_elapsed         | 314          |\n",
      "|    total_timesteps      | 37888        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037309814 |\n",
      "|    clip_fraction        | 0.000488     |\n",
      "|    clip_range           | 0.3          |\n",
      "|    entropy_loss         | -4.28        |\n",
      "|    explained_variance   | 0.528        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | -0.0384      |\n",
      "|    n_updates            | 360          |\n",
      "|    policy_gradient_loss | -0.00629     |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 0.00199      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1            |\n",
      "|    ep_rew_mean          | 0.864        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 122          |\n",
      "|    iterations           | 38           |\n",
      "|    time_elapsed         | 318          |\n",
      "|    total_timesteps      | 38912        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058765486 |\n",
      "|    clip_fraction        | 0.000684     |\n",
      "|    clip_range           | 0.3          |\n",
      "|    entropy_loss         | -4.28        |\n",
      "|    explained_variance   | 0.573        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | -0.0245      |\n",
      "|    n_updates            | 370          |\n",
      "|    policy_gradient_loss | -0.00674     |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 0.00194      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.01         |\n",
      "|    ep_rew_mean          | 0.859        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 123          |\n",
      "|    iterations           | 39           |\n",
      "|    time_elapsed         | 322          |\n",
      "|    total_timesteps      | 39936        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068452493 |\n",
      "|    clip_fraction        | 0.00146      |\n",
      "|    clip_range           | 0.3          |\n",
      "|    entropy_loss         | -4.27        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | -0.0295      |\n",
      "|    n_updates            | 380          |\n",
      "|    policy_gradient_loss | -0.00813     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 0.000164     |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1          |\n",
      "|    ep_rew_mean          | 0.86       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 123        |\n",
      "|    iterations           | 40         |\n",
      "|    time_elapsed         | 331        |\n",
      "|    total_timesteps      | 40960      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00312019 |\n",
      "|    clip_fraction        | 0          |\n",
      "|    clip_range           | 0.3        |\n",
      "|    entropy_loss         | -4.27      |\n",
      "|    explained_variance   | 0.417      |\n",
      "|    learning_rate        | 5e-05      |\n",
      "|    loss                 | -0.0128    |\n",
      "|    n_updates            | 390        |\n",
      "|    policy_gradient_loss | -0.00473   |\n",
      "|    std                  | 1          |\n",
      "|    value_loss           | 0.000848   |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1            |\n",
      "|    ep_rew_mean          | 0.864        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 125          |\n",
      "|    iterations           | 41           |\n",
      "|    time_elapsed         | 335          |\n",
      "|    total_timesteps      | 41984        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076086307 |\n",
      "|    clip_fraction        | 0.00264      |\n",
      "|    clip_range           | 0.3          |\n",
      "|    entropy_loss         | -4.27        |\n",
      "|    explained_variance   | 0.533        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | -0.0012      |\n",
      "|    n_updates            | 400          |\n",
      "|    policy_gradient_loss | -0.0101      |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 0.00121      |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 0.864       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 42          |\n",
      "|    time_elapsed         | 340         |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010573215 |\n",
      "|    clip_fraction        | 0.00928     |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -4.27       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0172     |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.0117     |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 0.000165    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1            |\n",
      "|    ep_rew_mean          | 0.865        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 127          |\n",
      "|    iterations           | 43           |\n",
      "|    time_elapsed         | 345          |\n",
      "|    total_timesteps      | 44032        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012146079 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.3          |\n",
      "|    entropy_loss         | -4.26        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | -0.0128      |\n",
      "|    n_updates            | 420          |\n",
      "|    policy_gradient_loss | -0.00146     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 0.000161     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1            |\n",
      "|    ep_rew_mean          | 0.864        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 127          |\n",
      "|    iterations           | 44           |\n",
      "|    time_elapsed         | 352          |\n",
      "|    total_timesteps      | 45056        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029650861 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.3          |\n",
      "|    entropy_loss         | -4.26        |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | -0.0297      |\n",
      "|    n_updates            | 430          |\n",
      "|    policy_gradient_loss | -0.00464     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.48e-06     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 0.864       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 129         |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 356         |\n",
      "|    total_timesteps      | 46080       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006170122 |\n",
      "|    clip_fraction        | 0.000684    |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -4.27       |\n",
      "|    explained_variance   | 0.459       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0123     |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.00595    |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 0.00121     |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1            |\n",
      "|    ep_rew_mean          | 0.862        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 130          |\n",
      "|    iterations           | 46           |\n",
      "|    time_elapsed         | 360          |\n",
      "|    total_timesteps      | 47104        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013090912 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.3          |\n",
      "|    entropy_loss         | -4.27        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | -0.0128      |\n",
      "|    n_updates            | 450          |\n",
      "|    policy_gradient_loss | -0.000612    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.16e-06     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1            |\n",
      "|    ep_rew_mean          | 0.864        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 131          |\n",
      "|    iterations           | 47           |\n",
      "|    time_elapsed         | 365          |\n",
      "|    total_timesteps      | 48128        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013005184 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.3          |\n",
      "|    entropy_loss         | -4.27        |\n",
      "|    explained_variance   | 0.564        |\n",
      "|    learning_rate        | 5e-05        |\n",
      "|    loss                 | -0.0153      |\n",
      "|    n_updates            | 460          |\n",
      "|    policy_gradient_loss | -0.00159     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 0.00095      |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 0.865       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 133         |\n",
      "|    iterations           | 48          |\n",
      "|    time_elapsed         | 368         |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010837474 |\n",
      "|    clip_fraction        | 0.00713     |\n",
      "|    clip_range           | 0.3         |\n",
      "|    entropy_loss         | -4.26       |\n",
      "|    explained_variance   | 0.564       |\n",
      "|    learning_rate        | 5e-05       |\n",
      "|    loss                 | -0.0221     |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | -0.00659    |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 0.000772    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 79\u001b[0m\n\u001b[1;32m      1\u001b[0m env \u001b[38;5;241m=\u001b[39m Parametric_envL(env_conf\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget\u001b[39m\u001b[38;5;124m\"\u001b[39m: target3SuperHad,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLagrange_time\u001b[39m\u001b[38;5;124m\"\u001b[39m:alpha})\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#    n_steps: 32\u001b[39;00m\n\u001b[1;32m      6\u001b[0m  \u001b[38;5;66;03m#   gamma: 0.999\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#    learning_rate: 0.0011773587234305126\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     73\u001b[0m  \u001b[38;5;66;03m#           n_epochs=20,ent_coef = 0.001,\u001b[39;00m\n\u001b[1;32m     74\u001b[0m  \u001b[38;5;66;03m#           verbose=1,tensorboard_log=\"./ppo_cartpole_tensorboard/\").learn(2000_000)\u001b[39;00m\n\u001b[1;32m     76\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mPPO\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMlpPolicy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.00005\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.99\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mn_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m            \u001b[49m\u001b[43mclip_range\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43ment_coef\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.003\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m---> 79\u001b[0m \u001b[43m            \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mtensorboard_log\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./ppo_cartpole_tensorboard/diss3/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3000_000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py:315\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[1;32m    308\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    313\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    314\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:300\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 300\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[1;32m    303\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:195\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    191\u001b[0m         \u001b[38;5;66;03m# Otherwise, clip the actions to avoid out of bound error\u001b[39;00m\n\u001b[1;32m    192\u001b[0m         \u001b[38;5;66;03m# as we are sampling from an unbounded Gaussian distribution\u001b[39;00m\n\u001b[1;32m    193\u001b[0m         clipped_actions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(actions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mlow, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mhigh)\n\u001b[0;32m--> 195\u001b[0m new_obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclipped_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mnum_envs\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# Give access to local variables\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:206\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:58\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# Avoid circular imports\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[0;32m---> 58\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews[env_idx], terminated, truncated, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;66;03m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx] \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/stable_baselines3/common/monitor.py:94\u001b[0m, in \u001b[0;36mMonitor.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneeds_reset:\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTried to step environment that needs reset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 94\u001b[0m observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewards\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mfloat\u001b[39m(reward))\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated:\n",
      "Cell \u001b[0;32mIn[16], line 110\u001b[0m, in \u001b[0;36mParametric_envL.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    108\u001b[0m  \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_times(action[\u001b[38;5;241m0\u001b[39m]),\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_times(action[\u001b[38;5;241m1\u001b[39m]),\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_angle(action[\u001b[38;5;241m2\u001b[39m])] \n\u001b[1;32m    109\u001b[0m  \u001b[38;5;66;03m#print(self.params)\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m  op \u001b[38;5;241m=\u001b[39m  \u001b[43msuter_layer_Super\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m###CHANGE: HERE\u001b[39;00m\n\u001b[1;32m    111\u001b[0m  opobs \u001b[38;5;241m=\u001b[39m suter_layer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# print(q.Qobj(op))\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# Upre = self.U\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 8\u001b[0m, in \u001b[0;36msuter_layer_Super\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msuter_layer_Super\u001b[39m(params):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# Here we can define a layer as an evolution without pulse control follow by a pulse control with fixed Rabi frequency\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m     U \u001b[38;5;241m=\u001b[39m UpulseSuper([params[\u001b[38;5;241m1\u001b[39m],params[\u001b[38;5;241m2\u001b[39m]]) \u001b[38;5;241m@\u001b[39m \u001b[43mUfreeSuper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m U\n",
      "Cell \u001b[0;32mIn[4], line 61\u001b[0m, in \u001b[0;36mUfreeSuper\u001b[0;34m(T)\u001b[0m\n\u001b[1;32m     59\u001b[0m num_qubit \u001b[38;5;241m=\u001b[39m num_wires\n\u001b[1;32m     60\u001b[0m L \u001b[38;5;241m=\u001b[39m Linb_APD(T1,T2, \u001b[38;5;241m0\u001b[39m, num_qubit)\n\u001b[0;32m---> 61\u001b[0m evo \u001b[38;5;241m=\u001b[39m \u001b[43mLinbladianExp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43mL\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnum_qubit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m#evo = qml.exp(H, -1j * T)\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m evo\n",
      "Cell \u001b[0;32mIn[3], line 182\u001b[0m, in \u001b[0;36mLinbladianExp\u001b[0;34m(t, h, L, num_qubit)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mLinbladianExp\u001b[39m(t,h,L,num_qubit):\n\u001b[0;32m--> 182\u001b[0m     sol \u001b[38;5;241m=\u001b[39m \u001b[43msp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mLinbladian\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43mL\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnum_qubit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sol\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/scipy/linalg/_matfuncs.py:350\u001b[0m, in \u001b[0;36mexpm\u001b[0;34m(A)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m s \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:  \u001b[38;5;66;03m# scaling needed\u001b[39;00m\n\u001b[1;32m    348\u001b[0m     Am[:\u001b[38;5;241m4\u001b[39m] \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m [[[\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m-\u001b[39ms)]], [[\u001b[38;5;241m4\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m-\u001b[39ms)]], [[\u001b[38;5;241m16\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m-\u001b[39ms)]], [[\u001b[38;5;241m64\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m-\u001b[39ms)]]]\n\u001b[0;32m--> 350\u001b[0m \u001b[43mpade_UV_calc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mAm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    351\u001b[0m eAw \u001b[38;5;241m=\u001b[39m Am[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m s \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:  \u001b[38;5;66;03m# squaring needed\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = Parametric_envL(env_conf={\"Target\": target3SuperHad,\"Lagrange_time\":alpha})\n",
    "\n",
    "\n",
    "\n",
    "#    n_steps: 32\n",
    " #   gamma: 0.999\n",
    "#    learning_rate: 0.0011773587234305126\n",
    "#    ent_coef: 5.272637170414138e-07\n",
    "#    clip_range: 0.2\n",
    "#    n_epochs: 10\n",
    "#    gae_lambda: 0.9\n",
    "#    max_grad_norm: 0.7\n",
    "#    vf_coef: 0.8595041935037702\n",
    "#    net_arch: small\n",
    "#    activation_fn: tanh\n",
    "\n",
    "\n",
    "#{'batch_size': 512, 'n_steps': 16, 'gamma': 0.9999,\n",
    "#'learning_rate': 0.0002994563320507711, \n",
    "#'ent_coef': 0.0032639297015691062, \n",
    "#'clip_range': 0.3, 'n_epochs': 5,\n",
    "#'gae_lambda': 0.95, 'max_grad_norm': 1,\n",
    "# 'vf_coef': 0.866843416840177,\n",
    "#'net_arch': 'medium', \n",
    "#'activation_fn': 'relu'}. Best is trial 3 with value: 2.626774.\n",
    "\n",
    "#policy, env, \n",
    "#learning_rate=0.0003, \n",
    "#n_steps=2048, \n",
    "#batch_size=64, \n",
    "#n_epochs=10, \n",
    "#gamma=0.99, \n",
    "#gae_lambda=0.95, \n",
    "#clip_range=0.2, \n",
    "#clip_range_vf=None, \n",
    "#normalize_advantage=True, \n",
    "#ent_coef=0.0, \n",
    "#vf_coef=0.5, \n",
    "#max_grad_norm=0.5, \n",
    "#use_sde=False, \n",
    "#sde_sample_freq=-1, \n",
    "#rollout_buffer_class=None, \n",
    "#rollout_buffer_kwargs=None, \n",
    "#target_kl=None, \n",
    "#stats_window_size=100, \n",
    "#tensorboard_log=None, \n",
    "#policy_kwargs=None, \n",
    "#verbose=0, \n",
    "#seed=None, \n",
    "#device='auto',\n",
    "#_init_setup_model=True)\n",
    "\n",
    "# Best so far: model = PPO(\"MlpPolicy\", env, gamma=0.9998,n_steps=1024,batch_size=64,clip_range=0.1, n_epochs=10,ent_coef=0.00, verbose=1).learn(4000_000)\n",
    "# with parameters: t = 7# number = 5\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#{'batch_size': 512, 'n_steps': 16, 'gamma': 0.9999,\n",
    "#'learning_rate': 0.0002994563320507711, \n",
    "#'ent_coef': 0.0032639297015691062, \n",
    "#'clip_range': 0.3, 'n_epochs': 5,\n",
    "#'gae_lambda': 0.95, 'max_grad_norm': 1,\n",
    "# 'vf_coef': 0.866843416840177,\n",
    "#'net_arch': 'medium', \n",
    "#'activation_fn': 'relu'}. Best is trial 3 with value: 2.626774.\n",
    "\n",
    "# Train the agent\n",
    "#MlpPolicy\n",
    "#model = PPO(\"MlpPolicy\", env,  \n",
    "#            learning_rate=0.0003,gamma=0.99,\n",
    " #           n_steps=2048,batch_size=64,clip_range=0.2,\n",
    " #           n_epochs=20,ent_coef = 0.001,\n",
    " #           verbose=1,tensorboard_log=\"./ppo_cartpole_tensorboard/\").learn(2000_000)\n",
    "\n",
    "model = PPO(\"MlpPolicy\", env, learning_rate=0.00005,\n",
    "            gamma=0.99,n_steps=1024,batch_size=256,\n",
    "            clip_range=0.3, n_epochs=10,ent_coef=0.003, \n",
    "            verbose = 1,tensorboard_log=\"./ppo_cartpole_tensorboard/diss3/\").learn(3000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = vec_env.reset()\n",
    "n_steps = 17\n",
    "for step in range(n_steps):\n",
    "    action, _ = model.predict(obs, deterministic= True)\n",
    "    print(f\"Step {step + 1}\")\n",
    "    print(\"Action: \", [action[0][i] for i in range(3)])\n",
    "    obs, reward, done, info = vec_env.step(action)\n",
    "    #print(\"obs=\", obs, \"reward=\", reward, \"done=\", done)\n",
    "    print(\"reward=\", reward,\"fidelity=\",info[0][\"Fidelity\"], \"done=\", done)\n",
    "    #vec_env.render()\n",
    "    if done:\n",
    "        # Note that the VecEnv resets automatically\n",
    "        # when a done signal is encountered\n",
    "        if reward > 0.99:\n",
    "            print(\"Goal reached\", \"Fidelity=\", info[0])\n",
    "        else:\n",
    "            print(\" Max number of layers\", \"Fidelity=\", info)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.18.0 at http://localhost:6006/ (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir ./ppo_cartpole_tensorboard/diss6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
