{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b4bccbd-22d4-4e1c-b867-709a886e4717",
   "metadata": {},
   "source": [
    "# Alessio Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a92b9cc-fdfb-4c2e-ba02-8ee9b3d92513",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import pennylane as qml\n",
    "\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "\n",
    "# Plotting and monitoring\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common import results_plotter\n",
    "from stable_baselines3.common.results_plotter import ts2xy, load_results\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "import os\n",
    "LOG_DIR = os.path.join( os.getcwd(), 'log')\n",
    "os.makedirs(LOG_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a274bc56-16ca-44f7-b6c9-c6a3c22e38da",
   "metadata": {},
   "source": [
    "## Chapter 1: Single quibt superposition state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "562bc668-9358-4c4f-abd3-cf8a4a0393ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y, Z = qml.PauliX, qml.PauliY, qml.PauliZ\n",
    "Id = qml.Identity(wires=[0]).matrix()\n",
    "\n",
    "def layer(params):\n",
    "    U = qml.RX(params[1],wires=0).matrix() @ qml.RZ(params[0],wires=0).matrix()\n",
    "    return U\n",
    "\n",
    "def profit(state,target_state):\n",
    "    return np.abs(np.dot(target_state.conj(), state))**2\n",
    "\n",
    "# Solution:\n",
    "# [0, np.pi/2], [np.pi/2, 0] = [2, 4], [4, 2]\n",
    "# [0, -np.pi/2], [-np.pi/2, 0] = [2, 0], [0, 2]\n",
    "# target_state = qml.Hadamard(wires=0).matrix() @ np.array([1,0])\n",
    "# state = layer([np.pi/2, 0]) @ layer([0, np.pi/2]) @ np.array([1,0])\n",
    "# profit(target_state, state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17bb677e-5b9b-4421-928c-43a9089b91dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "psi0 = np.array([1,0])\n",
    "\n",
    "class Parametric_env(gym.Env):\n",
    "    MAX_STEPS = 4\n",
    "    INFIDELITY_THRESHOLD = 1e-06\n",
    "\n",
    "    def __init__(self, env_conf):\n",
    "        self.target = env_conf[\"Target\"]\n",
    "        self.alpha = env_conf[\"Lagrange_time\"]\n",
    "\n",
    "        self.U = Id\n",
    "        self.psi=psi0\n",
    "        self.psi0=psi0\n",
    "        \n",
    "        self.action_space = gym.spaces.MultiDiscrete([5,5])\n",
    "        self.observation_space = gym.spaces.Box(low=-1, high=1, shape=(4,), dtype=np.float64)\n",
    "        \n",
    "    def _get_obs(self):\n",
    "        ob = np.concatenate([np.real(self.psi),np.imag(self.psi)])\n",
    "        return ob\n",
    "    \n",
    "    def reset(self,seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.U = Id\n",
    "        self.psi=psi0\n",
    "        self.fidelity = profit(self.psi,self.target)\n",
    "        self.count = 0\n",
    "        self.reward = 0\n",
    "        self.done = False\n",
    "        self.duration = 0\n",
    "        observation = self._get_obs()\n",
    "        self.info = {}\n",
    "\n",
    "        return observation,{}\n",
    "    \n",
    "    def _get_params(self,para):\n",
    "        az = 0\n",
    "    \n",
    "        if para==0:\n",
    "            az = -np.pi/2\n",
    "        elif para == 1:\n",
    "            az = -np.pi/4\n",
    "        elif para == 2:\n",
    "            az = 0\n",
    "        elif para == 3:\n",
    "            az = np.pi/4\n",
    "        elif para == 4:\n",
    "            az = np.pi/2\n",
    "        return az\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.done:\n",
    "            print(\"EPISODE DONE!!!\")\n",
    "        elif (self.count == self.MAX_STEPS):\n",
    "            self.done = True\n",
    "        else:\n",
    "            assert self.action_space.contains(action)\n",
    "            self.count += 1\n",
    "            \n",
    "        self.params = [self._get_params(action[0]),self._get_params(action[1])] \n",
    "        op = layer(self.params)\n",
    "\n",
    "        self.U = op @ self.U  \n",
    "        self.psi = self.U @ self.psi0\n",
    "\n",
    "        self.fidelity = profit(self.psi,self.target)\n",
    "        self.info = {\"Fidelity\": self.fidelity}\n",
    "\n",
    "        if self.done:\n",
    "            self.reward = self.fidelity\n",
    "        else:\n",
    "            self.reward = 0\n",
    "\n",
    "        observation = self._get_obs()\n",
    "        return [observation, self.reward, self.done,self.done, self.info]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9626d65-ee78-462d-ab30-a3e7784a5327",
   "metadata": {},
   "outputs": [],
   "source": [
    "rho_target = np.array([0,1])\n",
    "env = Parametric_env(env_conf={\"Target\": rho_target,\"Lagrange_time\":1})\n",
    "check_env(env, warn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e852e643-bd4e-43dd-96d5-8df94a1d1df6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.70710678, 0.70710678])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_gate = qml.Hadamard(wires=0).matrix()\n",
    "target_state = target_gate @ psi0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0da60f9e-9022-486b-8910-291e75533662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "vec_env_test = make_vec_env(lambda:Parametric_env(env_conf={\"Target\": target_state, \"Lagrange_time\":0})\n",
    ", n_envs=1)\n",
    "\n",
    "vec_env = make_vec_env(lambda:Parametric_env(env_conf={\"Target\": target_state, \"Lagrange_time\":0})\n",
    ", n_envs=2,seed=0, vec_env_cls=SubprocVecEnv)\n",
    "\n",
    "model = PPO(\"MlpPolicy\", vec_env, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b7fc885-2e64-4b12-8d34-5686c0e48d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward: 0.6401649999999999 +/- 0.00\n"
     ]
    }
   ],
   "source": [
    "mean_reward, std_reward = evaluate_policy(model, vec_env_test, n_eval_episodes=10)\n",
    "print(f\"Mean reward: {mean_reward} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e5db770-51d0-4779-a293-04c2cf63df9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5        |\n",
      "|    ep_rew_mean     | 0.519    |\n",
      "| time/              |          |\n",
      "|    fps             | 641      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 6        |\n",
      "|    total_timesteps | 4096     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5           |\n",
      "|    ep_rew_mean          | 0.52        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 456         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 17          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012180576 |\n",
      "|    clip_fraction        | 0.138       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.21       |\n",
      "|    explained_variance   | -1.22       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0239      |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0161     |\n",
      "|    value_loss           | 0.0822      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5           |\n",
      "|    ep_rew_mean          | 0.579       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 415         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011038981 |\n",
      "|    clip_fraction        | 0.151       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.19       |\n",
      "|    explained_variance   | 0.00903     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0115      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0195     |\n",
      "|    value_loss           | 0.0651      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5           |\n",
      "|    ep_rew_mean          | 0.609       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 392         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 41          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013475827 |\n",
      "|    clip_fraction        | 0.153       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.14       |\n",
      "|    explained_variance   | 0.0262      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0222     |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0202     |\n",
      "|    value_loss           | 0.0606      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 5          |\n",
      "|    ep_rew_mean          | 0.619      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 398        |\n",
      "|    iterations           | 5          |\n",
      "|    time_elapsed         | 51         |\n",
      "|    total_timesteps      | 20480      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01671819 |\n",
      "|    clip_fraction        | 0.174      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -3.09      |\n",
      "|    explained_variance   | 0.0219     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0147     |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | -0.0243    |\n",
      "|    value_loss           | 0.058      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5           |\n",
      "|    ep_rew_mean          | 0.697       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 404         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 60          |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013279121 |\n",
      "|    clip_fraction        | 0.147       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.04       |\n",
      "|    explained_variance   | 0.0284      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00231     |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0205     |\n",
      "|    value_loss           | 0.0522      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5           |\n",
      "|    ep_rew_mean          | 0.729       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 408         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 70          |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017682586 |\n",
      "|    clip_fraction        | 0.169       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.96       |\n",
      "|    explained_variance   | 0.0664      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0257     |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0256     |\n",
      "|    value_loss           | 0.044       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5           |\n",
      "|    ep_rew_mean          | 0.766       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 408         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 80          |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017000208 |\n",
      "|    clip_fraction        | 0.172       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.87       |\n",
      "|    explained_variance   | 0.0676      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0147      |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0259     |\n",
      "|    value_loss           | 0.0371      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5           |\n",
      "|    ep_rew_mean          | 0.836       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 410         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 89          |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017928604 |\n",
      "|    clip_fraction        | 0.178       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.79       |\n",
      "|    explained_variance   | 0.0702      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00735     |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0266     |\n",
      "|    value_loss           | 0.0301      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 5          |\n",
      "|    ep_rew_mean          | 0.86       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 411        |\n",
      "|    iterations           | 10         |\n",
      "|    time_elapsed         | 99         |\n",
      "|    total_timesteps      | 40960      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01465711 |\n",
      "|    clip_fraction        | 0.159      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.7       |\n",
      "|    explained_variance   | 0.0558     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00331   |\n",
      "|    n_updates            | 90         |\n",
      "|    policy_gradient_loss | -0.0235    |\n",
      "|    value_loss           | 0.0194     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5           |\n",
      "|    ep_rew_mean          | 0.885       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 410         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 109         |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016292516 |\n",
      "|    clip_fraction        | 0.169       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.58       |\n",
      "|    explained_variance   | 0.0651      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0342     |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0247     |\n",
      "|    value_loss           | 0.0155      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5           |\n",
      "|    ep_rew_mean          | 0.893       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 411         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 119         |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019192897 |\n",
      "|    clip_fraction        | 0.227       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.45       |\n",
      "|    explained_variance   | 0.0853      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0198     |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.03       |\n",
      "|    value_loss           | 0.0132      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5           |\n",
      "|    ep_rew_mean          | 0.932       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 412         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 129         |\n",
      "|    total_timesteps      | 53248       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018614858 |\n",
      "|    clip_fraction        | 0.214       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.31       |\n",
      "|    explained_variance   | 0.0831      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00805    |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0335     |\n",
      "|    value_loss           | 0.00949     |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x2627858bf90>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(50_000)\n",
    "# model.save(\"Simple_example\")\n",
    "# model_loaded = PPO.load(\"Simple_example\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "edafa242-48c3-42bd-ac13-bb4c41cf1126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward: 1.0 +/- 0.00\n"
     ]
    }
   ],
   "source": [
    "mean_reward, std_reward = evaluate_policy(model, vec_env_test, n_eval_episodes=10)\n",
    "print(f\"Mean reward: {mean_reward} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3c7b1b7-193d-4e61-bdad-2d15f12833a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1\n",
      "Action:  [[1 1]\n",
      " [3 2]]\n",
      "reward= [0 0]\n",
      "Fidelity: ({'Fidelity': np.float64(0.5000000000000001), 'TimeLimit.truncated': False}, {'Fidelity': np.float64(0.4999999999999999), 'TimeLimit.truncated': False})\n",
      "Step 2\n",
      "Action:  [[0 4]\n",
      " [1 3]]\n",
      "reward= [0 0]\n",
      "Fidelity: ({'Fidelity': np.float64(0.8535533905932742), 'TimeLimit.truncated': False}, {'Fidelity': np.float64(0.4999999999999999), 'TimeLimit.truncated': False})\n",
      "Step 3\n",
      "Action:  [[3 0]\n",
      " [3 0]]\n",
      "reward= [0 0]\n",
      "Fidelity: ({'Fidelity': np.float64(1.0000000000000004), 'TimeLimit.truncated': False}, {'Fidelity': np.float64(0.7499999999999999), 'TimeLimit.truncated': False})\n",
      "Step 4\n",
      "Action:  [[2 4]\n",
      " [1 3]]\n",
      "reward= [0 0]\n",
      "Fidelity: ({'Fidelity': np.float64(1.0000000000000009), 'TimeLimit.truncated': False}, {'Fidelity': np.float64(0.9267766952966368), 'TimeLimit.truncated': False})\n",
      "Step 5\n",
      "Action:  [[2 4]\n",
      " [2 2]]\n",
      "reward= [1.        0.9267767]\n",
      "Fidelity: ({'Fidelity': np.float64(1.0000000000000009), 'episode': {'r': 1.0, 'l': 5, 't': 136.604352}, 'TimeLimit.truncated': False, 'terminal_observation': array([ 0.65328148,  0.65328148, -0.27059805, -0.27059805])}, {'Fidelity': np.float64(0.9267766952966368), 'episode': {'r': 0.926777, 'l': 5, 't': 136.605347}, 'TimeLimit.truncated': False, 'terminal_observation': array([ 0.85355339,  0.5       ,  0.        , -0.14644661])})\n",
      "Goal reached Fidelity= ({'Fidelity': np.float64(1.0000000000000009), 'episode': {'r': 1.0, 'l': 5, 't': 136.604352}, 'TimeLimit.truncated': False, 'terminal_observation': array([ 0.65328148,  0.65328148, -0.27059805, -0.27059805])}, {'Fidelity': np.float64(0.9267766952966368), 'episode': {'r': 0.926777, 'l': 5, 't': 136.605347}, 'TimeLimit.truncated': False, 'terminal_observation': array([ 0.85355339,  0.5       ,  0.        , -0.14644661])})\n"
     ]
    }
   ],
   "source": [
    "obs = vec_env.reset()\n",
    "n_steps = 8\n",
    "for step in range(n_steps):\n",
    "    action, _ = model.predict(obs, deterministic= False)\n",
    "    print(f\"Step {step + 1}\")\n",
    "    print(\"Action: \", action)\n",
    "    obs, reward, done, info = vec_env.step(action)\n",
    "    print(\"reward=\", reward)\n",
    "    print(\"Fidelity:\", info)\n",
    "    if done.all():\n",
    "        if info[0][\"Fidelity\"] > 0.99:\n",
    "            print(\"Goal reached\", \"Fidelity=\", info)\n",
    "        else:\n",
    "            print(\" Max number of layers\", \"Fidelity=\", info[0][\"Fidelity\"],\"Fidelity=\",info[1][\"Fidelity\"])\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae49aa3e-a2d1-440d-b617-c4633830f125",
   "metadata": {},
   "source": [
    "### Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66a04d46-d493-47ed-a096-fd92ce67428f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
    "    def __init__(self, check_freq, log_dir, verbose=1):\n",
    "        \n",
    "        super().__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.log_dir = log_dir\n",
    "        self.save_path = os.path.join(log_dir, \"best_model\")\n",
    "        self.best_mean_reward = -np.inf\n",
    "\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "\n",
    "            # Retrieve training reward\n",
    "            x, y = ts2xy(load_results(self.log_dir), \"timesteps\")\n",
    "            if len(x) > 0:\n",
    "                # Mean training reward over the last 100 episodes\n",
    "                mean_reward = np.mean(y[-100:])\n",
    "                if self.verbose > 0:\n",
    "                    print(f\"Num timesteps: {self.num_timesteps}\")\n",
    "                    print(\n",
    "                        f\"Best mean reward: {self.best_mean_reward:.2f} - Last mean reward per episode: {mean_reward:.2f}\"\n",
    "                    )\n",
    "\n",
    "                # New best model, you could save the agent here\n",
    "                if mean_reward > self.best_mean_reward:\n",
    "                    self.best_mean_reward = mean_reward\n",
    "                    # Example for saving best model\n",
    "                    if self.verbose > 0:\n",
    "                        print(f\"Saving new best model to {self.save_path}.zip\")\n",
    "                    self.model.save(self.save_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19433e24-eba5-452b-b3da-87dd57084374",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Num timesteps: 1000\n",
      "Best mean reward: -inf - Last mean reward per episode: 0.53\n",
      "Saving new best model to C:\\Users\\Dennis Herb\\OneDrive\\2_Uni\\Doktor\\python_projects\\NVcenter\\development\\log\\best_model.zip\n",
      "Num timesteps: 2000\n",
      "Best mean reward: 0.53 - Last mean reward per episode: 0.51\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 5        |\n",
      "|    ep_rew_mean     | 0.49     |\n",
      "| time/              |          |\n",
      "|    fps             | 676      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 3        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "Num timesteps: 3000\n",
      "Best mean reward: 0.53 - Last mean reward per episode: 0.52\n",
      "Num timesteps: 4000\n",
      "Best mean reward: 0.53 - Last mean reward per episode: 0.50\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5           |\n",
      "|    ep_rew_mean          | 0.493       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 460         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010902762 |\n",
      "|    clip_fraction        | 0.101       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.21       |\n",
      "|    explained_variance   | -0.495      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0087      |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0138     |\n",
      "|    value_loss           | 0.0821      |\n",
      "-----------------------------------------\n",
      "Num timesteps: 5000\n",
      "Best mean reward: 0.53 - Last mean reward per episode: 0.59\n",
      "Saving new best model to C:\\Users\\Dennis Herb\\OneDrive\\2_Uni\\Doktor\\python_projects\\NVcenter\\development\\log\\best_model.zip\n",
      "Num timesteps: 6000\n",
      "Best mean reward: 0.59 - Last mean reward per episode: 0.55\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5           |\n",
      "|    ep_rew_mean          | 0.543       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 406         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014741715 |\n",
      "|    clip_fraction        | 0.181       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.19       |\n",
      "|    explained_variance   | 0.00172     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0119      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0243     |\n",
      "|    value_loss           | 0.061       |\n",
      "-----------------------------------------\n",
      "Num timesteps: 7000\n",
      "Best mean reward: 0.59 - Last mean reward per episode: 0.52\n",
      "Num timesteps: 8000\n",
      "Best mean reward: 0.59 - Last mean reward per episode: 0.59\n",
      "Saving new best model to C:\\Users\\Dennis Herb\\OneDrive\\2_Uni\\Doktor\\python_projects\\NVcenter\\development\\log\\best_model.zip\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 5         |\n",
      "|    ep_rew_mean          | 0.623     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 387       |\n",
      "|    iterations           | 4         |\n",
      "|    time_elapsed         | 21        |\n",
      "|    total_timesteps      | 8192      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0130983 |\n",
      "|    clip_fraction        | 0.17      |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -3.15     |\n",
      "|    explained_variance   | 0.0106    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 0.0153    |\n",
      "|    n_updates            | 30        |\n",
      "|    policy_gradient_loss | -0.0223   |\n",
      "|    value_loss           | 0.0634    |\n",
      "---------------------------------------\n",
      "Num timesteps: 9000\n",
      "Best mean reward: 0.59 - Last mean reward per episode: 0.62\n",
      "Saving new best model to C:\\Users\\Dennis Herb\\OneDrive\\2_Uni\\Doktor\\python_projects\\NVcenter\\development\\log\\best_model.zip\n",
      "Num timesteps: 10000\n",
      "Best mean reward: 0.62 - Last mean reward per episode: 0.63\n",
      "Saving new best model to C:\\Users\\Dennis Herb\\OneDrive\\2_Uni\\Doktor\\python_projects\\NVcenter\\development\\log\\best_model.zip\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 5          |\n",
      "|    ep_rew_mean          | 0.577      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 379        |\n",
      "|    iterations           | 5          |\n",
      "|    time_elapsed         | 26         |\n",
      "|    total_timesteps      | 10240      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01326113 |\n",
      "|    clip_fraction        | 0.137      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -3.1       |\n",
      "|    explained_variance   | -0.00322   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.00728    |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | -0.0176    |\n",
      "|    value_loss           | 0.0583     |\n",
      "----------------------------------------\n",
      "Num timesteps: 11000\n",
      "Best mean reward: 0.63 - Last mean reward per episode: 0.68\n",
      "Saving new best model to C:\\Users\\Dennis Herb\\OneDrive\\2_Uni\\Doktor\\python_projects\\NVcenter\\development\\log\\best_model.zip\n",
      "Num timesteps: 12000\n",
      "Best mean reward: 0.68 - Last mean reward per episode: 0.67\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5           |\n",
      "|    ep_rew_mean          | 0.615       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 372         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 33          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012889897 |\n",
      "|    clip_fraction        | 0.144       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.05       |\n",
      "|    explained_variance   | 0.0249      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00391     |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0224     |\n",
      "|    value_loss           | 0.0585      |\n",
      "-----------------------------------------\n",
      "Num timesteps: 13000\n",
      "Best mean reward: 0.68 - Last mean reward per episode: 0.67\n",
      "Num timesteps: 14000\n",
      "Best mean reward: 0.68 - Last mean reward per episode: 0.66\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5           |\n",
      "|    ep_rew_mean          | 0.665       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 364         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 39          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015625743 |\n",
      "|    clip_fraction        | 0.156       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.98       |\n",
      "|    explained_variance   | 0.0615      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.000969    |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0237     |\n",
      "|    value_loss           | 0.0526      |\n",
      "-----------------------------------------\n",
      "Num timesteps: 15000\n",
      "Best mean reward: 0.68 - Last mean reward per episode: 0.76\n",
      "Saving new best model to C:\\Users\\Dennis Herb\\OneDrive\\2_Uni\\Doktor\\python_projects\\NVcenter\\development\\log\\best_model.zip\n",
      "Num timesteps: 16000\n",
      "Best mean reward: 0.76 - Last mean reward per episode: 0.74\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 5          |\n",
      "|    ep_rew_mean          | 0.728      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 364        |\n",
      "|    iterations           | 8          |\n",
      "|    time_elapsed         | 44         |\n",
      "|    total_timesteps      | 16384      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01748204 |\n",
      "|    clip_fraction        | 0.165      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.91      |\n",
      "|    explained_variance   | 0.0689     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00472   |\n",
      "|    n_updates            | 70         |\n",
      "|    policy_gradient_loss | -0.0281    |\n",
      "|    value_loss           | 0.0501     |\n",
      "----------------------------------------\n",
      "Num timesteps: 17000\n",
      "Best mean reward: 0.76 - Last mean reward per episode: 0.73\n",
      "Num timesteps: 18000\n",
      "Best mean reward: 0.76 - Last mean reward per episode: 0.78\n",
      "Saving new best model to C:\\Users\\Dennis Herb\\OneDrive\\2_Uni\\Doktor\\python_projects\\NVcenter\\development\\log\\best_model.zip\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5           |\n",
      "|    ep_rew_mean          | 0.788       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 364         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 50          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012603767 |\n",
      "|    clip_fraction        | 0.135       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.84       |\n",
      "|    explained_variance   | 0.0686      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0429     |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0202     |\n",
      "|    value_loss           | 0.0351      |\n",
      "-----------------------------------------\n",
      "Num timesteps: 19000\n",
      "Best mean reward: 0.78 - Last mean reward per episode: 0.77\n",
      "Num timesteps: 20000\n",
      "Best mean reward: 0.78 - Last mean reward per episode: 0.80\n",
      "Saving new best model to C:\\Users\\Dennis Herb\\OneDrive\\2_Uni\\Doktor\\python_projects\\NVcenter\\development\\log\\best_model.zip\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 5          |\n",
      "|    ep_rew_mean          | 0.796      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 367        |\n",
      "|    iterations           | 10         |\n",
      "|    time_elapsed         | 55         |\n",
      "|    total_timesteps      | 20480      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01455183 |\n",
      "|    clip_fraction        | 0.152      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.76      |\n",
      "|    explained_variance   | 0.0647     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00952   |\n",
      "|    n_updates            | 90         |\n",
      "|    policy_gradient_loss | -0.022     |\n",
      "|    value_loss           | 0.0319     |\n",
      "----------------------------------------\n",
      "Num timesteps: 21000\n",
      "Best mean reward: 0.80 - Last mean reward per episode: 0.84\n",
      "Saving new best model to C:\\Users\\Dennis Herb\\OneDrive\\2_Uni\\Doktor\\python_projects\\NVcenter\\development\\log\\best_model.zip\n",
      "Num timesteps: 22000\n",
      "Best mean reward: 0.84 - Last mean reward per episode: 0.82\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5           |\n",
      "|    ep_rew_mean          | 0.844       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 367         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 61          |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015204588 |\n",
      "|    clip_fraction        | 0.162       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.66       |\n",
      "|    explained_variance   | 0.0665      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.016       |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0266     |\n",
      "|    value_loss           | 0.0315      |\n",
      "-----------------------------------------\n",
      "Num timesteps: 23000\n",
      "Best mean reward: 0.84 - Last mean reward per episode: 0.87\n",
      "Saving new best model to C:\\Users\\Dennis Herb\\OneDrive\\2_Uni\\Doktor\\python_projects\\NVcenter\\development\\log\\best_model.zip\n",
      "Num timesteps: 24000\n",
      "Best mean reward: 0.87 - Last mean reward per episode: 0.85\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5           |\n",
      "|    ep_rew_mean          | 0.852       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 367         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 66          |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014694168 |\n",
      "|    clip_fraction        | 0.139       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.56       |\n",
      "|    explained_variance   | 0.11        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0114     |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0257     |\n",
      "|    value_loss           | 0.0247      |\n",
      "-----------------------------------------\n",
      "Num timesteps: 25000\n",
      "Best mean reward: 0.87 - Last mean reward per episode: 0.87\n",
      "Saving new best model to C:\\Users\\Dennis Herb\\OneDrive\\2_Uni\\Doktor\\python_projects\\NVcenter\\development\\log\\best_model.zip\n",
      "Num timesteps: 26000\n",
      "Best mean reward: 0.87 - Last mean reward per episode: 0.88\n",
      "Saving new best model to C:\\Users\\Dennis Herb\\OneDrive\\2_Uni\\Doktor\\python_projects\\NVcenter\\development\\log\\best_model.zip\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5           |\n",
      "|    ep_rew_mean          | 0.882       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 369         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 72          |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014395819 |\n",
      "|    clip_fraction        | 0.156       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.45       |\n",
      "|    explained_variance   | 0.0325      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0147     |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0222     |\n",
      "|    value_loss           | 0.0153      |\n",
      "-----------------------------------------\n",
      "Num timesteps: 27000\n",
      "Best mean reward: 0.88 - Last mean reward per episode: 0.91\n",
      "Saving new best model to C:\\Users\\Dennis Herb\\OneDrive\\2_Uni\\Doktor\\python_projects\\NVcenter\\development\\log\\best_model.zip\n",
      "Num timesteps: 28000\n",
      "Best mean reward: 0.91 - Last mean reward per episode: 0.92\n",
      "Saving new best model to C:\\Users\\Dennis Herb\\OneDrive\\2_Uni\\Doktor\\python_projects\\NVcenter\\development\\log\\best_model.zip\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 5          |\n",
      "|    ep_rew_mean          | 0.908      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 369        |\n",
      "|    iterations           | 14         |\n",
      "|    time_elapsed         | 77         |\n",
      "|    total_timesteps      | 28672      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01391096 |\n",
      "|    clip_fraction        | 0.145      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.37      |\n",
      "|    explained_variance   | 0.125      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0295    |\n",
      "|    n_updates            | 130        |\n",
      "|    policy_gradient_loss | -0.0227    |\n",
      "|    value_loss           | 0.0114     |\n",
      "----------------------------------------\n",
      "Num timesteps: 29000\n",
      "Best mean reward: 0.92 - Last mean reward per episode: 0.90\n",
      "Num timesteps: 30000\n",
      "Best mean reward: 0.92 - Last mean reward per episode: 0.93\n",
      "Saving new best model to C:\\Users\\Dennis Herb\\OneDrive\\2_Uni\\Doktor\\python_projects\\NVcenter\\development\\log\\best_model.zip\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5           |\n",
      "|    ep_rew_mean          | 0.932       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 368         |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 83          |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011765924 |\n",
      "|    clip_fraction        | 0.121       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.28       |\n",
      "|    explained_variance   | 0.0777      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0367     |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0235     |\n",
      "|    value_loss           | 0.00647     |\n",
      "-----------------------------------------\n",
      "Num timesteps: 31000\n",
      "Best mean reward: 0.93 - Last mean reward per episode: 0.92\n",
      "Num timesteps: 32000\n",
      "Best mean reward: 0.93 - Last mean reward per episode: 0.94\n",
      "Saving new best model to C:\\Users\\Dennis Herb\\OneDrive\\2_Uni\\Doktor\\python_projects\\NVcenter\\development\\log\\best_model.zip\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5           |\n",
      "|    ep_rew_mean          | 0.956       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 365         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 89          |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013604721 |\n",
      "|    clip_fraction        | 0.151       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.17       |\n",
      "|    explained_variance   | 0.0898      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0543     |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0269     |\n",
      "|    value_loss           | 0.0064      |\n",
      "-----------------------------------------\n",
      "Num timesteps: 33000\n",
      "Best mean reward: 0.94 - Last mean reward per episode: 0.95\n",
      "Saving new best model to C:\\Users\\Dennis Herb\\OneDrive\\2_Uni\\Doktor\\python_projects\\NVcenter\\development\\log\\best_model.zip\n",
      "Num timesteps: 34000\n",
      "Best mean reward: 0.95 - Last mean reward per episode: 0.96\n",
      "Saving new best model to C:\\Users\\Dennis Herb\\OneDrive\\2_Uni\\Doktor\\python_projects\\NVcenter\\development\\log\\best_model.zip\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5           |\n",
      "|    ep_rew_mean          | 0.951       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 365         |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 95          |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013860734 |\n",
      "|    clip_fraction        | 0.153       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.05       |\n",
      "|    explained_variance   | 0.128       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0208     |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0246     |\n",
      "|    value_loss           | 0.00622     |\n",
      "-----------------------------------------\n",
      "Num timesteps: 35000\n",
      "Best mean reward: 0.96 - Last mean reward per episode: 0.95\n",
      "Num timesteps: 36000\n",
      "Best mean reward: 0.96 - Last mean reward per episode: 0.95\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5           |\n",
      "|    ep_rew_mean          | 0.968       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 363         |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 101         |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012374306 |\n",
      "|    clip_fraction        | 0.133       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.92       |\n",
      "|    explained_variance   | 0.13        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0571     |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0235     |\n",
      "|    value_loss           | 0.00363     |\n",
      "-----------------------------------------\n",
      "Num timesteps: 37000\n",
      "Best mean reward: 0.96 - Last mean reward per episode: 0.97\n",
      "Saving new best model to C:\\Users\\Dennis Herb\\OneDrive\\2_Uni\\Doktor\\python_projects\\NVcenter\\development\\log\\best_model.zip\n",
      "Num timesteps: 38000\n",
      "Best mean reward: 0.97 - Last mean reward per episode: 0.98\n",
      "Saving new best model to C:\\Users\\Dennis Herb\\OneDrive\\2_Uni\\Doktor\\python_projects\\NVcenter\\development\\log\\best_model.zip\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5           |\n",
      "|    ep_rew_mean          | 0.969       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 362         |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 107         |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013758926 |\n",
      "|    clip_fraction        | 0.135       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.8        |\n",
      "|    explained_variance   | 0.147       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0186     |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0241     |\n",
      "|    value_loss           | 0.00428     |\n",
      "-----------------------------------------\n",
      "Num timesteps: 39000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.97\n",
      "Num timesteps: 40000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.98\n",
      "Saving new best model to C:\\Users\\Dennis Herb\\OneDrive\\2_Uni\\Doktor\\python_projects\\NVcenter\\development\\log\\best_model.zip\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5           |\n",
      "|    ep_rew_mean          | 0.986       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 360         |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 113         |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014854633 |\n",
      "|    clip_fraction        | 0.127       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.66       |\n",
      "|    explained_variance   | 0.113       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0147     |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0236     |\n",
      "|    value_loss           | 0.00265     |\n",
      "-----------------------------------------\n",
      "Num timesteps: 41000\n",
      "Best mean reward: 0.98 - Last mean reward per episode: 0.99\n",
      "Saving new best model to C:\\Users\\Dennis Herb\\OneDrive\\2_Uni\\Doktor\\python_projects\\NVcenter\\development\\log\\best_model.zip\n",
      "Num timesteps: 42000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.98\n",
      "Num timesteps: 43000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.99\n",
      "Saving new best model to C:\\Users\\Dennis Herb\\OneDrive\\2_Uni\\Doktor\\python_projects\\NVcenter\\development\\log\\best_model.zip\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 5          |\n",
      "|    ep_rew_mean          | 0.985      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 358        |\n",
      "|    iterations           | 21         |\n",
      "|    time_elapsed         | 119        |\n",
      "|    total_timesteps      | 43008      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00944569 |\n",
      "|    clip_fraction        | 0.0758     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.57      |\n",
      "|    explained_variance   | 0.166      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0266    |\n",
      "|    n_updates            | 200        |\n",
      "|    policy_gradient_loss | -0.0172    |\n",
      "|    value_loss           | 0.00163    |\n",
      "----------------------------------------\n",
      "Num timesteps: 44000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.99\n",
      "Saving new best model to C:\\Users\\Dennis Herb\\OneDrive\\2_Uni\\Doktor\\python_projects\\NVcenter\\development\\log\\best_model.zip\n",
      "Num timesteps: 45000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.99\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5           |\n",
      "|    ep_rew_mean          | 0.987       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 357         |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 126         |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012119461 |\n",
      "|    clip_fraction        | 0.131       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.47       |\n",
      "|    explained_variance   | 0.212       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00901    |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.0204     |\n",
      "|    value_loss           | 0.00114     |\n",
      "-----------------------------------------\n",
      "Num timesteps: 46000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.99\n",
      "Num timesteps: 47000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 0.99\n",
      "Saving new best model to C:\\Users\\Dennis Herb\\OneDrive\\2_Uni\\Doktor\\python_projects\\NVcenter\\development\\log\\best_model.zip\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5           |\n",
      "|    ep_rew_mean          | 0.994       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 355         |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 132         |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013775135 |\n",
      "|    clip_fraction        | 0.128       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.34       |\n",
      "|    explained_variance   | 0.201       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0345     |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.02       |\n",
      "|    value_loss           | 0.00119     |\n",
      "-----------------------------------------\n",
      "Num timesteps: 48000\n",
      "Best mean reward: 0.99 - Last mean reward per episode: 1.00\n",
      "Saving new best model to C:\\Users\\Dennis Herb\\OneDrive\\2_Uni\\Doktor\\python_projects\\NVcenter\\development\\log\\best_model.zip\n",
      "Num timesteps: 49000\n",
      "Best mean reward: 1.00 - Last mean reward per episode: 0.99\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5           |\n",
      "|    ep_rew_mean          | 0.996       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 354         |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 138         |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013283427 |\n",
      "|    clip_fraction        | 0.13        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.3        |\n",
      "|    explained_variance   | 0.213       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.015       |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.0153     |\n",
      "|    value_loss           | 0.000993    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 50000\n",
      "Best mean reward: 1.00 - Last mean reward per episode: 1.00\n",
      "Saving new best model to C:\\Users\\Dennis Herb\\OneDrive\\2_Uni\\Doktor\\python_projects\\NVcenter\\development\\log\\best_model.zip\n",
      "Num timesteps: 51000\n",
      "Best mean reward: 1.00 - Last mean reward per episode: 1.00\n",
      "Saving new best model to C:\\Users\\Dennis Herb\\OneDrive\\2_Uni\\Doktor\\python_projects\\NVcenter\\development\\log\\best_model.zip\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5           |\n",
      "|    ep_rew_mean          | 0.996       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 353         |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 144         |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009849694 |\n",
      "|    clip_fraction        | 0.0675      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.25       |\n",
      "|    explained_variance   | 0.255       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0164     |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0114     |\n",
      "|    value_loss           | 0.000504    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 52000\n",
      "Best mean reward: 1.00 - Last mean reward per episode: 1.00\n",
      "Num timesteps: 53000\n",
      "Best mean reward: 1.00 - Last mean reward per episode: 1.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 5            |\n",
      "|    ep_rew_mean          | 0.995        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 351          |\n",
      "|    iterations           | 26           |\n",
      "|    time_elapsed         | 151          |\n",
      "|    total_timesteps      | 53248        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076397527 |\n",
      "|    clip_fraction        | 0.109        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.24        |\n",
      "|    explained_variance   | 0.3          |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0114      |\n",
      "|    n_updates            | 250          |\n",
      "|    policy_gradient_loss | -0.011       |\n",
      "|    value_loss           | 0.000364     |\n",
      "------------------------------------------\n",
      "Num timesteps: 54000\n",
      "Best mean reward: 1.00 - Last mean reward per episode: 1.00\n",
      "Num timesteps: 55000\n",
      "Best mean reward: 1.00 - Last mean reward per episode: 1.00\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 5         |\n",
      "|    ep_rew_mean          | 0.994     |\n",
      "| time/                   |           |\n",
      "|    fps                  | 350       |\n",
      "|    iterations           | 27        |\n",
      "|    time_elapsed         | 157       |\n",
      "|    total_timesteps      | 55296     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0113882 |\n",
      "|    clip_fraction        | 0.11      |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.21     |\n",
      "|    explained_variance   | 0.284     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.0112   |\n",
      "|    n_updates            | 260       |\n",
      "|    policy_gradient_loss | -0.0139   |\n",
      "|    value_loss           | 0.000508  |\n",
      "---------------------------------------\n",
      "Num timesteps: 56000\n",
      "Best mean reward: 1.00 - Last mean reward per episode: 1.00\n",
      "Saving new best model to C:\\Users\\Dennis Herb\\OneDrive\\2_Uni\\Doktor\\python_projects\\NVcenter\\development\\log\\best_model.zip\n",
      "Num timesteps: 57000\n",
      "Best mean reward: 1.00 - Last mean reward per episode: 1.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5           |\n",
      "|    ep_rew_mean          | 0.998       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 351         |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 163         |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007881278 |\n",
      "|    clip_fraction        | 0.0738      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.18       |\n",
      "|    explained_variance   | 0.2         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00353    |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.00784    |\n",
      "|    value_loss           | 0.000899    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 58000\n",
      "Best mean reward: 1.00 - Last mean reward per episode: 1.00\n",
      "Num timesteps: 59000\n",
      "Best mean reward: 1.00 - Last mean reward per episode: 1.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5           |\n",
      "|    ep_rew_mean          | 0.998       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 352         |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 168         |\n",
      "|    total_timesteps      | 59392       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012775121 |\n",
      "|    clip_fraction        | 0.118       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.14       |\n",
      "|    explained_variance   | 0.483       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.028      |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0113     |\n",
      "|    value_loss           | 0.000179    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 60000\n",
      "Best mean reward: 1.00 - Last mean reward per episode: 1.00\n",
      "Num timesteps: 61000\n",
      "Best mean reward: 1.00 - Last mean reward per episode: 0.99\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5           |\n",
      "|    ep_rew_mean          | 0.997       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 353         |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 173         |\n",
      "|    total_timesteps      | 61440       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013991471 |\n",
      "|    clip_fraction        | 0.0988      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | 0.386       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00854    |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.0102     |\n",
      "|    value_loss           | 0.000258    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 62000\n",
      "Best mean reward: 1.00 - Last mean reward per episode: 1.00\n",
      "Num timesteps: 63000\n",
      "Best mean reward: 1.00 - Last mean reward per episode: 1.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 5            |\n",
      "|    ep_rew_mean          | 0.999        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 354          |\n",
      "|    iterations           | 31           |\n",
      "|    time_elapsed         | 179          |\n",
      "|    total_timesteps      | 63488        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072509516 |\n",
      "|    clip_fraction        | 0.0493       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.08        |\n",
      "|    explained_variance   | 0.195        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0242       |\n",
      "|    n_updates            | 300          |\n",
      "|    policy_gradient_loss | -0.0094      |\n",
      "|    value_loss           | 0.00109      |\n",
      "------------------------------------------\n",
      "Num timesteps: 64000\n",
      "Best mean reward: 1.00 - Last mean reward per episode: 1.00\n",
      "Num timesteps: 65000\n",
      "Best mean reward: 1.00 - Last mean reward per episode: 1.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5           |\n",
      "|    ep_rew_mean          | 0.999       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 355         |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 184         |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040847465 |\n",
      "|    clip_fraction        | 0.112       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.02       |\n",
      "|    explained_variance   | 0.335       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0183     |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.0116     |\n",
      "|    value_loss           | 0.000176    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 66000\n",
      "Best mean reward: 1.00 - Last mean reward per episode: 1.00\n",
      "Num timesteps: 67000\n",
      "Best mean reward: 1.00 - Last mean reward per episode: 1.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5           |\n",
      "|    ep_rew_mean          | 1           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 356         |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 189         |\n",
      "|    total_timesteps      | 67584       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010408064 |\n",
      "|    clip_fraction        | 0.0941      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.99       |\n",
      "|    explained_variance   | 0.72        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0466     |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.0115     |\n",
      "|    value_loss           | 5.23e-05    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 68000\n",
      "Best mean reward: 1.00 - Last mean reward per episode: 1.00\n",
      "Num timesteps: 69000\n",
      "Best mean reward: 1.00 - Last mean reward per episode: 1.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5           |\n",
      "|    ep_rew_mean          | 1           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 357         |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 194         |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007834736 |\n",
      "|    clip_fraction        | 0.0742      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.977      |\n",
      "|    explained_variance   | 0.471       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00653    |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.00966    |\n",
      "|    value_loss           | 0.000196    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 70000\n",
      "Best mean reward: 1.00 - Last mean reward per episode: 1.00\n",
      "Num timesteps: 71000\n",
      "Best mean reward: 1.00 - Last mean reward per episode: 1.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 5            |\n",
      "|    ep_rew_mean          | 0.999        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 357          |\n",
      "|    iterations           | 35           |\n",
      "|    time_elapsed         | 200          |\n",
      "|    total_timesteps      | 71680        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0085333865 |\n",
      "|    clip_fraction        | 0.0772       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.965       |\n",
      "|    explained_variance   | 0.554        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00179      |\n",
      "|    n_updates            | 340          |\n",
      "|    policy_gradient_loss | -0.00652     |\n",
      "|    value_loss           | 0.000138     |\n",
      "------------------------------------------\n",
      "Num timesteps: 72000\n",
      "Best mean reward: 1.00 - Last mean reward per episode: 1.00\n",
      "Num timesteps: 73000\n",
      "Best mean reward: 1.00 - Last mean reward per episode: 1.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5           |\n",
      "|    ep_rew_mean          | 1           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 358         |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 205         |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005205917 |\n",
      "|    clip_fraction        | 0.0337      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.948      |\n",
      "|    explained_variance   | 0.59        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00601     |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.00481    |\n",
      "|    value_loss           | 0.000123    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 74000\n",
      "Best mean reward: 1.00 - Last mean reward per episode: 1.00\n",
      "Num timesteps: 75000\n",
      "Best mean reward: 1.00 - Last mean reward per episode: 1.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 5            |\n",
      "|    ep_rew_mean          | 1            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 358          |\n",
      "|    iterations           | 37           |\n",
      "|    time_elapsed         | 211          |\n",
      "|    total_timesteps      | 75776        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057287617 |\n",
      "|    clip_fraction        | 0.053        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.967       |\n",
      "|    explained_variance   | 0.548        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0271      |\n",
      "|    n_updates            | 360          |\n",
      "|    policy_gradient_loss | -0.00591     |\n",
      "|    value_loss           | 0.00014      |\n",
      "------------------------------------------\n",
      "Num timesteps: 76000\n",
      "Best mean reward: 1.00 - Last mean reward per episode: 1.00\n",
      "Num timesteps: 77000\n",
      "Best mean reward: 1.00 - Last mean reward per episode: 1.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5           |\n",
      "|    ep_rew_mean          | 0.998       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 358         |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 216         |\n",
      "|    total_timesteps      | 77824       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029521197 |\n",
      "|    clip_fraction        | 0.0435      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.966      |\n",
      "|    explained_variance   | 0.565       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0356     |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.00327    |\n",
      "|    value_loss           | 0.000127    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 78000\n",
      "Best mean reward: 1.00 - Last mean reward per episode: 1.00\n",
      "Num timesteps: 79000\n",
      "Best mean reward: 1.00 - Last mean reward per episode: 1.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5           |\n",
      "|    ep_rew_mean          | 0.999       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 359         |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 221         |\n",
      "|    total_timesteps      | 79872       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012084858 |\n",
      "|    clip_fraction        | 0.0923      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.976      |\n",
      "|    explained_variance   | 0.494       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.021      |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.0115     |\n",
      "|    value_loss           | 0.000182    |\n",
      "-----------------------------------------\n",
      "Num timesteps: 80000\n",
      "Best mean reward: 1.00 - Last mean reward per episode: 1.00\n",
      "Num timesteps: 81000\n",
      "Best mean reward: 1.00 - Last mean reward per episode: 1.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 5            |\n",
      "|    ep_rew_mean          | 1            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 359          |\n",
      "|    iterations           | 40           |\n",
      "|    time_elapsed         | 227          |\n",
      "|    total_timesteps      | 81920        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068186815 |\n",
      "|    clip_fraction        | 0.0735       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.993       |\n",
      "|    explained_variance   | 0.662        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.000976     |\n",
      "|    n_updates            | 390          |\n",
      "|    policy_gradient_loss | -0.00708     |\n",
      "|    value_loss           | 8.79e-05     |\n",
      "------------------------------------------\n",
      "Num timesteps: 82000\n",
      "Best mean reward: 1.00 - Last mean reward per episode: 1.00\n",
      "Num timesteps: 83000\n",
      "Best mean reward: 1.00 - Last mean reward per episode: 1.00\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 5           |\n",
      "|    ep_rew_mean          | 1           |\n",
      "| time/                   |             |\n",
      "|    fps                  | 359         |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 233         |\n",
      "|    total_timesteps      | 83968       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017904563 |\n",
      "|    clip_fraction        | 0.0674      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.02       |\n",
      "|    explained_variance   | 0.75        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0242     |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.00615    |\n",
      "|    value_loss           | 4.9e-05     |\n",
      "-----------------------------------------\n",
      "Num timesteps: 84000\n",
      "Best mean reward: 1.00 - Last mean reward per episode: 1.00\n",
      "Num timesteps: 85000\n",
      "Best mean reward: 1.00 - Last mean reward per episode: 1.00\n",
      "Num timesteps: 86000\n",
      "Best mean reward: 1.00 - Last mean reward per episode: 1.00\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 5            |\n",
      "|    ep_rew_mean          | 1            |\n",
      "| time/                   |              |\n",
      "|    fps                  | 360          |\n",
      "|    iterations           | 42           |\n",
      "|    time_elapsed         | 238          |\n",
      "|    total_timesteps      | 86016        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071500377 |\n",
      "|    clip_fraction        | 0.093        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.05        |\n",
      "|    explained_variance   | 0.613        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00465     |\n",
      "|    n_updates            | 410          |\n",
      "|    policy_gradient_loss | -0.00794     |\n",
      "|    value_loss           | 0.000121     |\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "vec_env = Parametric_env(env_conf={\"Target\": target,\"Lagrange_time\":alpha})\n",
    "\n",
    "vec_env = Monitor(vec_env, LOG_DIR)\n",
    "callback = SaveOnBestTrainingRewardCallback(check_freq=1000, log_dir=LOG_DIR)\n",
    "\n",
    "model_monitor = PPO(\"MlpPolicy\", vec_env, verbose=1)\n",
    "model_monitor.learn(total_timesteps=50_000, callback=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27046cb-b558-4609-aa56-5b223ced31f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(values, window):\n",
    "    weights = np.repeat(1.0, window) / window\n",
    "    return np.convolve(values, weights, \"valid\")\n",
    "\n",
    "def plot_results(log_dir, title=\"Learning Curve\"):\n",
    "\n",
    "    x, y = ts2xy(load_results(log_dir), \"timesteps\")\n",
    "    y = moving_average(y, window=50)\n",
    "    # Truncate x\n",
    "    x = x[len(x) - len(y) :]\n",
    "\n",
    "    fig = plt.figure(title)\n",
    "    plt.plot(x, y)\n",
    "    plt.xlabel(\"Number of Timesteps\")\n",
    "    plt.ylabel(\"Rewards\")\n",
    "    plt.title(title + \" Smoothed\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1fb193-ad8b-4e46-bc8a-1b0a7eed0a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(LOG_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eaf6bb3-2eaf-4473-ae1e-9a9cf17b14e3",
   "metadata": {},
   "source": [
    "## Chapter 2: Theory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fff2b5-a7ba-4908-95e0-60d17bc0d02c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
